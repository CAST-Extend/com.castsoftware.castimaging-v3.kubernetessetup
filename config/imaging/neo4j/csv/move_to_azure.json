[
    {
        "name": "Move to Azure",
        "id": 1202283,
        "category": "START",
        "branchType": null,
        "rationales": {
            "Azure": "Very specific move to cloud recommendations for customers targeting Azure Platform"
        },
        "remediations": {},
        "populations": {},
        "description": "",
        "impacts": [],
        "effort": null,
        "parents": [],
        "outputTable": null,
        "outputGraph": null,
        "children": [
            1202284,
            1202294
        ]
    },
    {
        "name": "IMS Transactions",
        "id": 1202257,
        "category": "BRANCHING-STEP",
        "branchType": null,
        "rationales": {
            "Agnostic": "Analyzing the existing IMS transactions that you want to expose as web services is a very important step to Understand the input parameters, data formats, and expected responses.\nThis will also help to Determine the approach for transforming IMS transactions into web services: \nOptions include:\nWrap IMS Transactions as Web Services using tools or frameworks to create wrappers around IMS transactions and expose them as web services.\nUse Integration Middleware: Employ middleware solutions like IBM Integration Bus (IIB) or Apache Camel to bridge IMS and web services.\n\nNote also that Existing IMS programs will require code change to handle web service communication\n\n"
        },
        "remediations": {},
        "populations": {},
        "description": "",
        "impacts": [],
        "effort": null,
        "parents": [
            1202310
        ],
        "outputTable": null,
        "outputGraph": null,
        "children": [
            1202271,
            1202272
        ]
    },
    {
        "name": "IMS MFS control blocks.",
        "id": 1202261,
        "category": "BRANCHING-STEP",
        "branchType": null,
        "rationales": {
            "Agnostic": "Analyzing the existing IMS applications and identifying all Message Format Services used in the application and their interaction with different programs is a very important step before starting Migration. \n\nThis allows you to evaluate the required adjustments that address the specification of the chosen emulation solution provider to ensure compatibility, performance, and ongoing support for your specific use case\n\nNote also that IMS programs will require code change to handle the integration of the emulation solution\n\n\n\n"
        },
        "remediations": {},
        "populations": {
            "Mainframe": "search for nodes with type is one of (CICS_MAP, CICS_BMS, CICS_MAPSET, CAST_IMS_MessageFormatService, CAST_IMS_MessageInputDescriptor, CAST_IMS_MessageOutputDescriptor, CAST_CICS_MapPrototype, CAST_CICS_MapSetPrototype) + one callee per screen"
        },
        "description": "This rule checks any IMS Message Format Service, IMS Message Input Descriptor or IMS Message Output Descriptor called one single program",
        "impacts": [
            "retire",
            "rehost"
        ],
        "effort": null,
        "parents": [
            1202321
        ],
        "outputTable": null,
        "outputGraph": null,
        "children": [
            1202262,
            1202263,
            1202275
        ]
    },
    {
        "name": "IMS MFS control blocks  replaced with modern data formats",
        "id": 1202278,
        "category": "BRANCHING-STEP",
        "branchType": null,
        "rationales": {
            "Agnostic": "As part of phasing out of the mainframe IMS/DC transaction processing system in favor of modern web applications, Message formats defined by IMS MFS control blocks can be replaced with modern data formats, such as JSON or XML, for communication between a web application and ....\nThe first step of this modernization path consists of analyzing the existing IMS MFS message formats. Understand the structure, data types, and communication protocols used by IMS. \n\nThis will help design the JSON or XML data structure that will replace the IMS MFS message formats. Define how each piece of information will be represented in the new format.\n\nNote also that MS programs should be Modified to be able to generate or consume messages to use the new JSON or XML data format. This may involve updating COBOL or other programming languages used in IMS."
        },
        "remediations": {},
        "populations": {
            "Mainframe": "search for nodes with type is one of (CICS_MAP, CICS_BMS, CICS_MAPSET, CAST_IMS_MessageFormatService, CAST_IMS_MessageInputDescriptor, CAST_IMS_MessageOutputDescriptor, CAST_CICS_MapPrototype, CAST_CICS_MapSetPrototype) + one callee per screen"
        },
        "description": "This rule checks any IMS Message Format Service, IMS Message Input Descriptor or IMS Message Output Descriptor called one single program",
        "impacts": [
            "retire",
            "rehost"
        ],
        "effort": null,
        "parents": [
            1202310
        ],
        "outputTable": null,
        "outputGraph": null,
        "children": [
            1202263,
            1202279,
            1202280,
            1202281,
            1202282
        ]
    },
    {
        "name": "Data Base Migration to Azure Services",
        "id": 1202284,
        "category": "BRANCHING-STEP",
        "branchType": "CHOICE",
        "rationales": {
            "Azure": "When considering mainframe modernization with Azure, Data migration to Azure data services can be a suitable starting point.\nData migration is, also, a critical step in the overall modernization process, as it establishes the foundation for application transformation\n\nAzure services like Data Factory and AzCopy load data into Azure databases and Azure data storage. You can also use third-party solutions and custom-loading solutions to load data.\n\nAzure provides many managed data storage solutions:\n\nDatabases:\nAzure SQL Database\nAzure Database for PostgreSQL\nAzure Cosmos DB\nAzure Database for MySQL\nAzure Database for MariaDB\nAzure SQL Managed Instance\n\nStorage:\nAzure Data Lake Storage\nAzure Blob Storage\nAzure services use the modernized data tier for computing, analytics, storage, and networking.\n\n",
            "Amazon Web Services": "When considering mainframe modernization to AWS, Data migration to AWS data services can be a suitable starting point.\nData migration is, also, critical step in the overall modernization process, as it establishes the foundation for application transformation\n\nAWS provides services for the full data life cycle, from ingestion, to processing, storage, analysis, visualization, and automation. Migrating mainframe data from the mainframe’s relational, hierarchical, or legacy file-based data stores to agile AWS data lakes, data warehouses, or data stores allows quick integrate with various analytics and machine learning tools, enabling you to gain valuable insights from your data while keeping mainframe workloads."
        },
        "remediations": {},
        "populations": {},
        "description": "",
        "impacts": [],
        "effort": null,
        "parents": [
            1202283
        ],
        "outputTable": null,
        "outputGraph": null,
        "children": [
            1202285,
            1202289,
            1202290
        ]
    },
    {
        "name": "DB2 DataBase to Azure Database for PostgreSQL",
        "id": 1202285,
        "category": "BRANCHING-STEP",
        "branchType": null,
        "rationales": {
            "Azure": "Before migrating DB2 Database to Azure Database for PostgreSQL, the following checks need to be manually fixed since not supported on the target DB service\n",
            "Amazon Web Services": "Before migrating DB2 Database to Postgres on AWS Aurora, follwing checks need to be manual fixed since not supported on target DB service"
        },
        "remediations": {},
        "populations": {
            "Mainframe": "All checks of the TC: 61044"
        },
        "description": "",
        "impacts": [
            "replatform"
        ],
        "effort": null,
        "parents": [
            1202284
        ],
        "outputTable": null,
        "outputGraph": null,
        "children": [
            1202286,
            1202287,
            1202288
        ]
    },
    {
        "name": "DB2 DataBase to Azure Database for PostgreSQL : Data Type Remediation Required",
        "id": 1202286,
        "category": "BRANCHING-STEP",
        "branchType": null,
        "rationales": {
            "Agnostic": "Data Type is not supported by the target Database service. Automatic migration and conversion are not possible. Manual conversion with a dedicated equivalent Data Type is required."
        },
        "remediations": {},
        "populations": {
            "Mainframe": "TC 61044 + tag Data Type Remediation"
        },
        "description": "This rule checks unsupported data types when moving from DB2 to Azure Database for PostgreSQL",
        "impacts": [
            "replatform",
            "review"
        ],
        "effort": "moderate",
        "parents": [
            1202285
        ],
        "outputTable": {
            "template": "violations-table",
            "table": {
                "head": [
                    "NAME",
                    "TYPE",
                    "OCCURRENCES"
                ],
                "body": null
            }
        },
        "outputGraph": {
            "template": "violations-graph"
        },
        "children": [
            1106052,
            1106054,
            1106058,
            1106064,
            1106066,
            1106068,
            1106070
        ]
    },
    {
        "name": "DB2 DataBase to Azure Database for PostgreSQL: Object Type Remediation required",
        "id": 1202287,
        "category": "BRANCHING-STEP",
        "branchType": null,
        "rationales": {
            "Agnostic": "Target Database does not provide the support of the Object Type and sometimes does not provide a directly comparable alternative.  Automatic migration and conversion are not possible. However, other functions can be used as workarounds under certain conditions."
        },
        "remediations": {},
        "populations": {
            "Mainframe": "TC 61044 + tag  Syntax Remediation"
        },
        "description": "This rule checks unsupported object types when moving from DB2 to Azure Database for PostgreSQL",
        "impacts": [
            "replatform",
            "review"
        ],
        "effort": "high",
        "parents": [
            1202285
        ],
        "outputTable": {
            "template": "violations-table",
            "table": {
                "head": [
                    "NAME",
                    "TYPE",
                    "OCCURRENCES"
                ],
                "body": null
            }
        },
        "outputGraph": {
            "template": "violations-graph"
        },
        "children": [
            1106104
        ]
    },
    {
        "name": "DB2 DataBase to Azure Database for PostgreSQL: Syntax Remediation required",
        "id": 1202288,
        "category": "BRANCHING-STEP",
        "branchType": null,
        "rationales": {
            "Agnostic": "Used Syntax is not supported by the target Database service. Automatic migration and conversion are not possible. Manual conversion with dedicated equivalent syntax is required."
        },
        "remediations": {},
        "populations": {
            "Mainframe": "TC 61044 + tag  Syntax Remediation"
        },
        "description": "This rule checks unsupported syntaxes moving from DB2 to Azure Database for PostgreSQL",
        "impacts": [
            "replatform",
            "review"
        ],
        "effort": "low",
        "parents": [
            1202285
        ],
        "outputTable": {
            "template": "violations-table",
            "table": {
                "head": [
                    "NAME",
                    "TYPE",
                    "OCCURRENCES"
                ],
                "body": null
            }
        },
        "outputGraph": {
            "template": "violations-graph"
        },
        "children": [
            1106072,
            1106074,
            1106076,
            1106078,
            1106080,
            1106082,
            1106084,
            1106090,
            1106092,
            1106094,
            1106096,
            1106100,
            1106102,
            1106106,
            1106108
        ]
    },
    {
        "name": "IMS-DB Candidate To Azure Relation Database migration",
        "id": 1202289,
        "category": "BRANCHING-STEP",
        "branchType": null,
        "rationales": {
            "Azure": "IMS DB is a hierarchical database management system commonly used in mainframe environments that can be migrated to relational databases in cloud-based systems. \n\nPBS(Program Specification Block) and PCB are the control blocks used by IMS system to manage data access and processing. They are used by IMS to establish communication between a program and the IMS database.\nThey also include details such as segment names, segment lengths, segment occurrence groups, and relationships between segments. \n\nWhen Moving to Azure, IMS DB can be migrated to Azure Relational Database Service or any x86-64 platform database engine hosted on Azure. The x86-64 platform database engine could be any of Azure SQL Database, SQL Server, DB2 LUW, or Oracle Database on Azure.\n",
            "Amazon Web Services": "IMS DB is a hierarchical database management system commonly used in mainframe environments that can be migrated to relational databases in cloud-based systems. \n\nPBS (Program Specification Block) and PCB are the control blocks used by IMS system to manage data access and processing. They are used by IMS to establish communication between a program and the IMS database.\nThey also include details such as segment names, segment lengths, segment occurrence groups, and relationships between segments. \n\nWhen Moving to AWS, IMS DB can be migrated to Amazon Relational Database Service or any x86-64 platform database engine hosted on AWS. The x86-64 platform database engine could be any of Amazon Aurora PostgreSQL, SQL Server, Oracle, DB2 LUW, or MariaDB."
        },
        "remediations": {},
        "populations": {},
        "description": "This rule allows to identify all IMS DB segements of you application, the access layers ( PSB and PC) in additional to Cobol Programs accesssing this Data base.",
        "impacts": [
            "refactor"
        ],
        "effort": null,
        "parents": [
            1202284
        ],
        "outputTable": null,
        "outputGraph": null,
        "children": [
            1202085,
            1202086,
            1202087
        ]
    },
    {
        "name": "Oracle DataBase to Azure Database for PostgreSQL",
        "id": 1202290,
        "category": "BRANCHING-STEP",
        "branchType": null,
        "rationales": {
            "Azure": "Before migrating an Oracle Database to Azure Database for PostgreSQL, the following checks need to be manually fixed since not supported on the target DB service\n\n\n"
        },
        "remediations": {},
        "populations": {},
        "description": "",
        "impacts": [
            "replatform"
        ],
        "effort": null,
        "parents": [
            1202284
        ],
        "outputTable": null,
        "outputGraph": null,
        "children": [
            1202291,
            1202292,
            1202293
        ]
    },
    {
        "name": "Oracle Database to Azure Database for PostgreSQL: Data Type Remediation",
        "id": 1202291,
        "category": "BRANCHING-STEP",
        "branchType": null,
        "rationales": {
            "Azure": "Recommendations to re-platform Oracle Database to Azure Database for PostgreSQL that will require data type remediation."
        },
        "remediations": {},
        "populations": {
            "Mainframe": "Check if any violation of rules with TC 61045 and with tag AIP-M2C-DataType"
        },
        "description": "This rule checks all unsupported Data types used in your Data Base when moving from Oracle to Azure Database for PostgreSQL",
        "impacts": [
            "replatform"
        ],
        "effort": "moderate",
        "parents": [
            1202290
        ],
        "outputTable": {
            "template": "violations-table",
            "table": {
                "head": [
                    "NAME",
                    "TYPE",
                    "OCCURRENCES"
                ],
                "body": null
            }
        },
        "outputGraph": {
            "template": "violations-graph"
        },
        "children": [
            1106020
        ]
    },
    {
        "name": "Oracle Database to Azure Database for PostgreSQL: Object Type Remediation",
        "id": 1202292,
        "category": "BRANCHING-STEP",
        "branchType": null,
        "rationales": {
            "Azure": "Recommendations to re-platform Oracle Database to Azure Database for PostgreSQL that will require object type remediation."
        },
        "remediations": {},
        "populations": {
            "Mainframe": "Check if any violation of rules with TC 61045 and with tag AIP-M2C-DataType"
        },
        "description": "This rule checks all unsupported Data types used in your data base when moving from Oracle to Azure Database for PostgreSQL",
        "impacts": [
            "replatform"
        ],
        "effort": "high",
        "parents": [
            1202290
        ],
        "outputTable": {
            "template": "violations-table",
            "table": {
                "head": [
                    "NAME",
                    "TYPE",
                    "OCCURRENCES"
                ],
                "body": null
            }
        },
        "outputGraph": {
            "template": "violations-graph"
        },
        "children": [
            1106006,
            1106008,
            1106010,
            1106012,
            1106014,
            1106036,
            1106040,
            1106042,
            1106044
        ]
    },
    {
        "name": "Oracle Database to Azure Database for PostgreSQL: Syntax Remediation required",
        "id": 1202293,
        "category": "BRANCHING-STEP",
        "branchType": null,
        "rationales": {
            "Azure": "Recommendations to re-platform Oracle Database to Azure Database for PostgreSQL that will require syntax change remediation."
        },
        "remediations": {},
        "populations": {
            "SQL": "Check if any violation for rules with TC 61045 and with tag AIP-M2C-Syntax",
            "Mainframe": "Check if any violation for rules with TC 61045 and with tag AIP-M2C-Syntax"
        },
        "description": "This rule checks unsupported syntaxes when moving from Oracle to Azure Database for PostgreSQL",
        "impacts": [
            "replatform"
        ],
        "effort": "low",
        "parents": [
            1202290
        ],
        "outputTable": {
            "template": "violations-table",
            "table": {
                "head": [
                    "NAME",
                    "TYPE",
                    "OCCURRENCES"
                ],
                "body": null
            }
        },
        "outputGraph": {
            "template": "violations-graph"
        },
        "children": [
            1106002,
            1106022,
            1106038
        ]
    },
    {
        "name": "Mainframe modernization with Azure",
        "id": 1202294,
        "category": "BRANCHING-STEP",
        "branchType": "CHOICE",
        "rationales": {
            "Azure": "Mainframe modernization with Azure refers to the process of migrating or transforming legacy mainframe applications and workloads to the cloud-based infrastructure and services provided by Azure",
            "Amazon Web Services": "Mainframe modernization with AWS refers to the process of migrating or transforming legacy mainframe applications and workloads to the cloud-based infrastructure and services provided by Amazon Web Services (AWS)."
        },
        "remediations": {},
        "populations": {},
        "description": "",
        "impacts": [],
        "effort": null,
        "parents": [
            1202283
        ],
        "outputTable": null,
        "outputGraph": null,
        "children": [
            1202295,
            1202319
        ]
    },
    {
        "name": "Mainframe To Azure by rehosting/replatforming your application on Azure virtual machines",
        "id": 1202295,
        "category": "BRANCHING-STEP",
        "branchType": "CHOICE",
        "rationales": {
            "Azure": "Mainframe applications can be deployed on Azure virtual machines using a Middleware Emulation solution. \nMany Emulation solutions are available such as Microfocus, tmaxsoft, ... make it easy to lift and shift existing IBM zSeries mainframe applications to Azure using a no-code approach. \n\nThis modernization approach involves migrating mainframe workloads to the cloud using the technology stack of emulation solutions. While this approach offers several benefits, it also presents some challenges and it requires a deep understanding of the legacy codebase and the ability to map mainframe functionality to modern cloud services.\n\nBefore starting the migration, it's crucial to assess the compatibility of used Communication, data transfer, and  Third-Party Products/tools protocols and utilities with the new Emulation solution.  \nThis evaluation helps also estimate migration costs and plan it effectively.\n\nData migration from mainframe systems to the cloud can be complex due to differences in data formats, storage architectures, and database technologies. Ensuring compatibility and preserving data integrity during the migration process requires careful planning.\n\nCode changes and/or refactoring are necessary to integrate with differing Emulation utility interfaces, and cloud-native integration services or when modernizing the data store and data access along the way.",
            "Amazon Web Services": "Mainframe applications can be deployed on Azure virtual machines using a Middleware Emulation solution. \nMany emulation solutions are available such as Microfocus, tmaxsoft, ... make it easy to lift and shift existing IBM zSeries mainframe applications to Aws using a no-code approach. \n\nThis modernization approach involves migrating mainframe workloads to the cloud using the technology stack of emulation solutions. While this approach offers several benefits, it also presents some challenges and it requires a deep understanding of the legacy codebase and the ability to map mainframe functionality to modern cloud services.\n\nBefore starting the migration, it's crucial to assess the compatibility of used Communication, data transfer, and  Third-Party Products/tools protocols and utilities with new Emulation solution.  \nThis evaluation helps also estimate migration costs and plan it effectively.\n\nData migration from mainframe systems to the cloud can be complex due to differences in data formats, storage architectures, and database technologies. Ensuring compatibility and preserving data integrity during the migration process requires careful planning.\n\nMinimal Code changes and/or refactoring are necessary to integrate with differing Emulation utility interfaces, and cloud-native integration services or when modernizing the data store and data access along the way."
        },
        "remediations": {},
        "populations": {},
        "description": "This rule checks all point of interest that need to be identfied and reviewed as part of a modernization project or migration to the cloud",
        "impacts": [],
        "effort": null,
        "parents": [
            1202294
        ],
        "outputTable": null,
        "outputGraph": null,
        "children": [
            1202296,
            1202300,
            1202301,
            1202305,
            1202306,
            1202307,
            1202308,
            1202309,
            1202311,
            1202312,
            1202313,
            1202321
        ]
    },
    {
        "name": "CICS Transactions processing system to be Rehosted on Azure Virtual Machine",
        "id": 1202296,
        "category": "BRANCHING-STEP",
        "branchType": null,
        "rationales": {
            "Azure": "Azure VMs can run mainframe emulation environments and compilers that support lift-and-shift scenarios. Common server components that you can emulate include online transaction process (OLTP), batch, and data ingestion systems;\n\nWhen moving CICS application to Azure, emulators for mainframe transaction processing (TP) monitors can run as infrastructure as a service (IaaS) using virtual machines (VMs) on Azure. The web servers can also implement screen handling and form functionality. Combine this approach with database APIs, such as ActiveX Data Objects (ADO), Open Database Connectivity (ODBC), and Java Database Connectivity (JDBC) for data access and transactions.\n\n\nSome specification and configuration options of CICS applications on the Z/OS environment should be reviewed and updated before the integration of emulators for mainframe transaction processing (TP) monitors on Azure VMs\n\nEvaluating the existing CICS application will help you assess the complexity of the migration project, and plan for necessary adjustments that address these specific requirements.",
            "Amazon Web Services": "Emulation solutions such as Microfocus, tmaxsoft, ... used to deploy Mainframe applications on Aws EC2, usually provide the support of CICS online transaction system and all CICS services, such as Transactions, Temporary Storage Queues, Temporary Data Queues or files acces ...\n\nSome specification and configuration options should be reviewed and updated before the integration of different CICS components into the new platform. \n\nEvaluating the existing CICS application will help you assess the complexity of the migration project, and plan for necessary adjustments that address these specific requirements.\n\n\n"
        },
        "remediations": {},
        "populations": {
            "Mainframe": "search for nodes with type is one of (CICS_MAP, CICS_BMS, CICS_MAPSET, CAST_IMS_MessageFormatService, CAST_IMS_MessageInputDescriptor, CAST_IMS_MessageOutputDescriptor, CAST_CICS_MapPrototype, CAST_CICS_MapSetPrototype) + callees to see which programs linked to these screens"
        },
        "description": "",
        "impacts": [
            "retire"
        ],
        "effort": null,
        "parents": [
            1202295
        ],
        "outputTable": null,
        "outputGraph": null,
        "children": [
            1202297,
            1202298,
            1202299
        ]
    },
    {
        "name": "CICS Green Screens",
        "id": 1202297,
        "category": "BRANCHING-STEP",
        "branchType": null,
        "rationales": {
            "Azure": "When moving CICS application to Azure, emulators for mainframe transaction processing (TP) monitors can run as infrastructure as a service (IaaS) using virtual machines (VMs) on Azure. The web servers can also implement screen handling and form functionality.\n\n\nIdentifying CICS Green Screens: Map, Mapset and Map definition is important to evaluate the review scope  to set the specific requirements ",
            "Amazon Web Services": "Emulation solutions such as Microfocus, tmaxsoft, ... used to deploy Mainframe applications on Aws EC2, usually provide the support of the CICS online transaction system.\n\nIdentifying CICS Map, Mapset and Map definition is important to evaluate reviewing scope to set the specific requirements before the integration into the new AWS platform. \n\n"
        },
        "remediations": {},
        "populations": {
            "Mainframe": " (CICS_MAP, CICS_BMS, CICS_MAPSET) and linked program ( Monitor link) "
        },
        "description": "This rule checks any CICS Map definition, CICS Mapset and CICS Map and Cobol program monitoring them",
        "impacts": [
            "retire",
            "rehost"
        ],
        "effort": "low",
        "parents": [
            1202296
        ],
        "outputTable": null,
        "outputGraph": null,
        "children": [
            1202211,
            1202212
        ]
    },
    {
        "name": "CICS Transactions",
        "id": 1202298,
        "category": "BRANCHING-STEP",
        "branchType": null,
        "rationales": {
            "Azure": "When moving CICS application to Azure, emulators for mainframe transaction processing (TP) monitors can run as infrastructure as a service (IaaS) using virtual machines (VMs) on Azure. The web servers can also implement screen handling and form functionality\n\nAnalyzing the existing CIS applications and identifying all CICS Transactions used in the application and their interaction with different programs is a very important step before starting Migration. \n\nThis allows you to evaluate the required adjustments that address the specification of the chosen emulation solution provider to ensure compatibility, performance, and ongoing support for your specific use case\n\nNote also that CICS Transactional programs will require code change to handle the integration of the emulation solution",
            "Amazon Web Services": "Transaction processing monitors like Tuxedo, BEA WebLogic Tuxedo, and other similar middleware can be used to host CICS applications on AWS EC2. They provide transaction management capabilities and can be configured to run CICS-compatible applications.\n\nIdentifying CICS transactions is important to evaluate the review scope to set the specific requirements "
        },
        "remediations": {},
        "populations": {},
        "description": "",
        "impacts": [
            "rehost"
        ],
        "effort": "low",
        "parents": [
            1202296
        ],
        "outputTable": null,
        "outputGraph": null,
        "children": [
            1202214,
            1202215
        ]
    },
    {
        "name": "CICS transient Data",
        "id": 1202299,
        "category": "BRANCHING-STEP",
        "branchType": null,
        "rationales": {
            "Azure": "When rehosting CICS application on Azure VMs, Consider using queue management software or messaging systems on Azure VMs to manage the replicated transient data queues. Azure services like Azure Service Bus or Azure Storage Queues could be suitable options.\n\n\n\nIdentifying CICS transient data and all Cobol Programs using transient Data is important since many changes will be required to integrate the replication services\n\n\n\n",
            "Amazon Web Services": "When rehosting CICS application on AWS EC2, consider using middleware or messaging systems on AWS EC2 to manage the replicated transient data queues. AWS services like Amazon SQS (Simple Queue Service) or Amazon MQ could be suitable options.\n\nIdentifying CICS transient data and all Cobol Programs using transient Data is important since many changes will be required to integrate the replication services"
        },
        "remediations": {},
        "populations": {
            "Mainframe": ""
        },
        "description": "",
        "impacts": [],
        "effort": null,
        "parents": [
            1202296
        ],
        "outputTable": null,
        "outputGraph": null,
        "children": [
            1202217,
            1202218
        ]
    },
    {
        "name": "Cobol Programs candidate for Azure function conversion",
        "id": 1202300,
        "category": "BRANCHING-STEP",
        "branchType": null,
        "rationales": {
            "Azure": "It's possible to increase agility and cost efficiency further by targeting serverless Azure functions\n\nNot only is the monolith broken down into separate services, but the services become smaller functions with no need to manage servers or containers. With Azure Function, there’s no charge when the code is not running.\n\nNot all programs are good use cases for Azure Function. Technical characteristics make Azure Function better suited for short-lived lightweight stateless functions. For this reason, some services are deployed in Azure Function while others are still deployed in containers or elastic compute.\n\nFor example, long-running batch processing cannot run in Azure Function but they can run in containers. Online transactions or batch-specific short functions, on the other hand, can run in Azure function.\n\n\nAzure WebJobs or Logic App can be also used for this kind of conversion ",
            "Amazon Web Services": "It's possible to increase agility and cost efficiency further by targeting serverless Lambda functions\n\nNot only is the monolith broken down into separate services, but the services become smaller functions with no need to manage servers or containers. With Aws Lambda Function, there’s no charge when the code is not running.\n\nNot all programs are good use cases for Lambda Function. Technical characteristics make Lambda Function better suited for short-lived lightweight stateless functions. For this reason, some services are deployed in Azure Function while others are still deployed in containers or elastic compute.\n\nFor example, long-running batch processing cannot run in Lambda Function but they can run in containers. Online transactions or batch-specific short functions, on the other hand, can run in Lambda.\n\n\n"
        },
        "remediations": {},
        "populations": {
            "Mainframe": "Cobol program belonging to one single transaction : \nMATCH (n:CDW {InternalType:'CAST_COBOL_SavedProgram'})-[p:Property]->(:ObjectProperty {Id:'1102000'}) where p.value in ['1', '2'] return n\n\n\nThe Entry Point of the transaction should have the property 'Number of Transaction' = 1\n\n"
        },
        "description": "The goal of this rule is to detect Cobol Program belonging to isolated transactions: These programs are good candidates for a manual rewrite as an AWS Lambda function or a standalone micro-Service",
        "impacts": [
            "replatform",
            "review"
        ],
        "effort": "high",
        "parents": [
            1202295,
            1202319
        ],
        "outputTable": null,
        "outputGraph": null,
        "children": [
            1202001,
            1202027,
            1202181
        ]
    },
    {
        "name": "Cobol Programs to be moved on Azure Virtual Machine : Syntax Checks",
        "id": 1202301,
        "category": "BRANCHING-STEP",
        "branchType": null,
        "rationales": {
            "Azure": "Some Cobol statements specific to the z/OS environment when used in a cloud environment can be error and/or security-prone. So, it will be very recommended to review them and fix issues before rehosting the Mainframe application on Azure Cloud Platform. \n\nIdentifying the unsupported or risky syntaxes beforehand can help organizations estimate the cost of the required code changes and/or refactoring and plan accordingly.\n",
            "Amazon Web Services": "Mainframe applications can be deployed on Aws EC2 using a Middleware Emulation. \nSome Cobol statements specific to the z/OS environment when used in a cloud environment can be error and/or security prone. So, it will be very recommended to review them and fixes issues before rehosting the Mainframe application on any Cloud Platform. \n\nIdentifying the unsupported or risky syntaxes beforehand can help organizations estimate the cost of the required code changes and/or refactoring and plan accordingly.\n\n\n\n\n"
        },
        "remediations": {},
        "populations": {},
        "description": "Identify all Cobol Statements and Synataxes that need to be reviewed before migration to the cloud",
        "impacts": [
            "rehost",
            "replatform",
            "review"
        ],
        "effort": null,
        "parents": [
            1202295
        ],
        "outputTable": null,
        "outputGraph": null,
        "children": [
            1202302,
            1202303
        ]
    },
    {
        "name": "Cobol Syntax Checks -  Security checks before migration",
        "id": 1202302,
        "category": "BRANCHING-STEP",
        "branchType": null,
        "rationales": {
            "Agnostic": "Cobol Statements and Syntaxes that may lead to security issues need to be reviewed before starting the migration to the cloud.\n"
        },
        "remediations": {},
        "populations": {
            "Mainframe": "8480: http://rulesmanager/#:-4:255\n5062: http://rulesmanager/#:-4:5w \n5072: http://rulesmanager/#:-4:61"
        },
        "description": "Identifying Cobol Statements Leading to security issues",
        "impacts": [
            "rehost",
            "replatform",
            "review"
        ],
        "effort": "high",
        "parents": [
            1202301
        ],
        "outputTable": null,
        "outputGraph": null,
        "children": [
            5062,
            5072,
            8480
        ]
    },
    {
        "name": "Cobol Syntax Checks -  z/OS environment  Specific",
        "id": 1202303,
        "category": "BRANCHING-STEP",
        "branchType": null,
        "rationales": {
            "Agnostic": "Some Cobol Syntax are specific to the z/OS environment and need to be reviewed before moving to Cloud Environment. if not Compilation error will be raised after the migration\n\n\n"
        },
        "remediations": {},
        "populations": {
            "Mainframe": "5144: http://rulesmanager/#:-4:6h\nTo be defined in rules Manager"
        },
        "description": "Identify all Cobol Statements and Synataxes that need to be reviewed before migration to the cloud",
        "impacts": [
            "review",
            "rehost"
        ],
        "effort": "low",
        "parents": [
            1202301
        ],
        "outputTable": null,
        "outputGraph": null,
        "children": [
            5060,
            5144
        ]
    },
    {
        "name": "Technologies Detection",
        "id": 1202304,
        "category": "BRANCHING-STEP",
        "branchType": null,
        "rationales": {
            "Agnostic": "The presence of the Focus, Assembler, Easytreive, Rexx code influences the migration strategy."
        },
        "remediations": {},
        "populations": {},
        "description": "",
        "impacts": [],
        "effort": null,
        "parents": [
            1202319
        ],
        "outputTable": null,
        "outputGraph": null,
        "children": [
            1202078,
            1202079,
            1202080,
            1202095
        ]
    },
    {
        "name": "Sequential Files Storage Candidate for Systems file service",
        "id": 1202305,
        "category": "BRANCHING-STEP",
        "branchType": null,
        "rationales": {
            "Azure": "Migrating Sequential datasets to sequential or indexed files stored in a file system is a recommended option for Mainframe data migration.\nFor example, Generation date groups (GDGs) files should be migrated to files on Azure that use a naming convention and filename extensions that provide similar functionality to GDGs.\n\n\n",
            "Amazon Web Services": "Migrating Sequential datasets to sequential or indexed files stored in a file system is a recommended option for Mainframe data migration.\n"
        },
        "remediations": {},
        "populations": {},
        "description": "",
        "impacts": [
            "replatform",
            "review"
        ],
        "effort": null,
        "parents": [
            1202295,
            1202319
        ],
        "outputTable": null,
        "outputGraph": null,
        "children": [
            1202014,
            1202015,
            1202030,
            1202048
        ]
    },
    {
        "name": "Mainframe Security Tools to Azure Native Secrurity Service",
        "id": 1202306,
        "category": "BRANCHING-STEP",
        "branchType": null,
        "rationales": {
            "Agnostic": "Mainframe applications often have specific security requirements. Identifying third-party security products early in the migration process allows organizations to assess their compatibility with the target environment and ensure seamless integration. It helps identify tools that need to be replaced by equivalent cloud-native services with likely corresponding code or configuration adaptations\n\nIdentifying third-party security products early allows also the migration team to assess the scope and complexity of required changes in application code and configuration to integrate new equivalent cloud services/tools"
        },
        "remediations": {},
        "populations": {},
        "description": "",
        "impacts": [],
        "effort": null,
        "parents": [
            1202295,
            1202319
        ],
        "outputTable": null,
        "outputGraph": null,
        "children": [
            1202035,
            1202037
        ]
    },
    {
        "name": "Mainframe Scheduler to Cloud Scheduler",
        "id": 1202307,
        "category": "BRANCHING-STEP",
        "branchType": null,
        "rationales": {
            "Agnostic": "The scheduler plays a critical role in the operation of mainframe applications. If the scheduler is not compatible with the cloud environment or if the migration is not properly planned, it can cause operational disruptions, increased downtime, and loss of revenue. Knowing the used scheduler can help in assessing and mitigating risks associated with the migration\n\nDifferent schedulers have different features and capabilities. Knowing which scheduler is being used in the mainframe application can help choose an equivalent cloud native scheduler with similar functionality.\n\nIdentifying the used scheduler and all JCL jobs, JCL procedures, and eventually Cobol Programs handled and monitored by this scheduler allows the migration team to assess the scope and complexity of the scheduling/workflow automation \ninfrastructure changes required to migrate to the cloud environment. So they can plan for all necessary changes and code refactoring to integrate the new cloud service replicating the used schedulers.\n\n\n\n\n"
        },
        "remediations": {},
        "populations": {},
        "description": "Identify the used Scheduler and all JCL jobs and procedures involved by this scheduler.\nIdentify Programs launched by these jobs",
        "impacts": [],
        "effort": null,
        "parents": [
            1202295,
            1202319
        ],
        "outputTable": null,
        "outputGraph": null,
        "children": [
            1202031,
            1202032,
            1202033,
            1202034
        ]
    },
    {
        "name": "Mainframe Printing and output management tools to Cloud Printing service",
        "id": 1202308,
        "category": "BRANCHING-STEP",
        "branchType": null,
        "rationales": {
            "Agnostic": "Mainframe printing and output management tools are often specific to the mainframe environment, and may not be compatible with cloud-based services. Identifying IBM or Independent Software Vendor printing and/or output management tools used on the z/OS environment before migration helps in determining if they need to be replaced or updated, and if planning for any necessary changes.\n\n\n"
        },
        "remediations": {},
        "populations": {},
        "description": "This rule we identify the usage of IBM or Independent Software Vendor printing or and output management too",
        "impacts": [],
        "effort": null,
        "parents": [
            1202295,
            1202319
        ],
        "outputTable": null,
        "outputGraph": null,
        "children": [
            1202038
        ]
    },
    {
        "name": "Mainframe Data Transfer Protocols to Cloud-Native File Transfer family",
        "id": 1202309,
        "category": "BRANCHING-STEP",
        "branchType": null,
        "rationales": {
            "Agnostic": "Mainframe applications often use proprietary data transfer protocols that may not be compatible with the protocols used in the cloud environment. Identifying the protocols beforehand can help ensure that the data can be transferred smoothly and without errors.\n\nWhen moving to the cloud platform, these protocols are usually replaced with cloud-native data transfer services or protocols. Minimal Code changes or refactoring are necessary to integrate the equivalent cloud-native service.\n\n"
        },
        "remediations": {},
        "populations": {},
        "description": "Identify JCL jobs and Procedures doing data and/or file transfer \nIdentify Programs launched by these jobs",
        "impacts": [
            "rehost",
            "review",
            "replatform"
        ],
        "effort": null,
        "parents": [
            1202295,
            1202319
        ],
        "outputTable": null,
        "outputGraph": null,
        "children": [
            1202036,
            1202039,
            1202040,
            1202041
        ]
    },
    {
        "name": "IMS Transactions processing system to be replaced with Web application",
        "id": 1202310,
        "category": "BRANCHING-STEP",
        "branchType": null,
        "rationales": {
            "Azure": "Transitioning to Azure necessitates the phasing out of mainframe green screens in favor of modern web applications.",
            "Amazon Web Services": "Transitioning to AWS necessitates the phasing out of mainframe green screens in favor of modern web applications.\n\n\nTransitioning to AWS necessitates the phasing out of the mainframe IMS transaction processing system in favor of modern web applications.\n\nEvaluate the existing IMS application is very important to \n- Identify the scope of the rewrite: which parts of the application need to be rewritten, which can be replaced \n- The user-facing application, the IMS Message Format Service is modernized to an Angular web",
            "Gougle Cloud Platform": "Transitioning to Google Cloud necessitates the phasing out of mainframe green screens in favor of modern web applications."
        },
        "remediations": {},
        "populations": {
            "Mainframe": "search for nodes with type is one of (CICS_MAP, CICS_BMS, CICS_MAPSET, CAST_IMS_MessageFormatService, CAST_IMS_MessageInputDescriptor, CAST_IMS_MessageOutputDescriptor, CAST_CICS_MapPrototype, CAST_CICS_MapSetPrototype) + callees to see which programs linked to these screens"
        },
        "description": "",
        "impacts": [
            "retire"
        ],
        "effort": null,
        "parents": [
            1202319
        ],
        "outputTable": null,
        "outputGraph": null,
        "children": [
            1202257,
            1202278
        ]
    },
    {
        "name": "Files Storage Candidate for Cloud Relational Database",
        "id": 1202311,
        "category": "BRANCHING-STEP",
        "branchType": null,
        "rationales": {
            "Azure": "Mainframes support several types of physical data sets. These data sets can create various migration challenges, depending on your target application and architecture.\n\nMost Data sets files cannot be kept as is and must be replaced/converted into a relational database in the target platform. Identifying data sets before migration helps identify the ones that require conversion, and ensures that the appropriate conversion steps are taken into consideration including the review of the data access logic of the current application.\n\n\nWhen moving to Azure, VSAM and other flat files, use Indexed Sequential Access Method (ISAM) flat files are usually migrated to Azure SQL Database, SQL Server, DB2 LUW, or Oracle.",
            "Amazon Web Services": "Mainframes support several types of physical data sets. These data sets can create various migration challenges, depending on your target application and architecture.\n\nMost Data sets files cannot be kept as is and must be replaced/converted into a relational database in the target platform. Identifying data sets before migration helps identify the ones that require conversion, and ensures that the appropriate conversion steps are taken into consideration including the review of the data access logic of the current application.\n\nWhen moving to AWS, Dataset files as VSAM which used to store and manage data in sequential order are, usually, replicated to Amazon RDS \nhttps://docs.aws.amazon.com/prescriptive-guidance/latest/patterns/migrate-and-replicate-vsam-files-to-amazon-rds-or-amazon-msk-using-connect-from-precisely.html\n"
        },
        "remediations": {},
        "populations": {},
        "description": "This set of rules will check the different data set files in the Mainframe application used for data Storage",
        "impacts": [
            "replatform"
        ],
        "effort": null,
        "parents": [
            1202295,
            1202319
        ],
        "outputTable": null,
        "outputGraph": null,
        "children": [
            1202029,
            1202042,
            1202043,
            1202051,
            1202064,
            1202065,
            1202093
        ]
    },
    {
        "name": "Cobol Programs using IBM MQ communication Protocol",
        "id": 1202312,
        "category": "BRANCHING-STEP",
        "branchType": null,
        "rationales": {
            "Azure": "Identifying communication protocols for mainframe applications is an important step in the cloud migration process because it helps ensure that the application can continue to communicate effectively with other systems and applications once it is moved to the cloud.\n\nMessage queues are a common way for different applications to exchange data and communicate with each other in a mainframe environment. When migrating the application to the cloud, it's very important to identify any dependencies on specific message queue protocol(s), and evaluate the adherence of the application with this communication protocol.\nthis is very helpful to identify the replication service in a target cloud environment \n\nWhen moving to Azure, IBM MQ system is usually replicated with Azure Queue Storage Service or other MQ-specific connectors. All Cobol programs using IBM MQ utilities, publishing or consuming messages will require a change/review of the connection APIs with the new cloud messaging service\n\n\nhttps://learn.microsoft.com/en-us/azure/architecture/example-scenario/mainframe/integrate-ibm-message-queues-azure\n\n\n\n",
            "Amazon Web Services": "Identifying communication protocols for mainframe applications is an important step in the cloud migration process because it helps ensure that the application can continue to communicate effectively with other systems and applications once it is moved to the cloud.\n\nMessage queues are a common way for different applications to exchange data and communicate with each other in a mainframe environment. When migrating the application to the cloud, it's very important to identify any dependencies on specific message queue protocol(s), and evaluate the adherence of the application with this communication protocol.\nthis is very helpful to identify the replication service in a target cloud environment \n\n\nWhen moving to AWS, the IBM MQ system is usually replicated with Amazon MQ Service. All Cobol programs using IBM MQ utilities, publishing or consuming messages will require a minimum change/review of the connection APIs with the new cloud messaging service",
            "Gougle Cloud Platform": "Identifying communication protocols for mainframe applications is an important step in the cloud migration process because it helps ensure that the application can continue to communicate effectively with other systems and applications once it is moved to the cloud.\n\nMessage queues are a common way for different applications to exchange data and communicate with each other in a mainframe environment. When migrating the application to the cloud, it's very important to identify any dependencies on specific message queue protocol(s), and evaluate the adherence of the application with this communication protocol.\nthis is very helpful to identify the replication service in a target cloud environment dates are needed to ensure smooth communication between the mainframe application and other systems in the cloud.\n\nWhen moving to GCP, IBM MQ system is usually replicated with Google Cloud Pub/Sub service. All Cobol programs using IBM MQ utilities, publishing or consuming messages will require a minimum change/review of the connection APIs with the new cloud messaging service\n\n"
        },
        "remediations": {},
        "populations": {
            "Mainframe": "Cobol Programs Calling MQ utilities: InternalType=CAST_COBOL_SavedProgram calling MQ utilities"
        },
        "description": "Identify all Cobol Programs using IBM MQSeries communication protocol.",
        "impacts": [
            "replatform",
            "review",
            "rehost"
        ],
        "effort": null,
        "parents": [
            1202295,
            1202319
        ],
        "outputTable": null,
        "outputGraph": null,
        "children": [
            1202020,
            1202049,
            1202050
        ]
    },
    {
        "name": "Cobol Programs using \"Program to Program\" communication Protocols",
        "id": 1202313,
        "category": "BRANCHING-STEP",
        "branchType": null,
        "rationales": {
            "Agnostic": "Identifying communication protocols for mainframe applications is an important step in the cloud migration process because it helps ensure that the application can continue to communicate effectively with other systems and applications once it is moved to the cloud.\n\nBy identifying the communication protocols used by the mainframe application, you can assess whether they are compatible with the cloud environment and determine if any replication with equivalent cloud-native services or any updates are needed to ensure smooth communication between the mainframe application and other systems in the cloud."
        },
        "remediations": {},
        "populations": {},
        "description": "",
        "impacts": [],
        "effort": null,
        "parents": [
            1202295,
            1202319
        ],
        "outputTable": null,
        "outputGraph": null,
        "children": [
            1202018,
            1202019
        ]
    },
    {
        "name": "CICS Transient Data",
        "id": 1202314,
        "category": "BRANCHING-STEP",
        "branchType": null,
        "rationales": {
            "Azure": "CICS® transient data queue services provide a generalized queueing facility. Data can be queued (stored) for subsequent internal or external processing. You can apply the following functions to selected data, specified in an application:\n\nWrite data to a transient data queue (EXEC CICS WRITEQ TD)\nRead data from a transient data queue (EXEC CICS READQ TD)\nDelete an Intra partition transient data queue (EXEC CICS DELETEQ TD)\n\n\nAs part of phasing out of the mainframe CICS transaction processing system, the Temporary Storage Queues and temporary Data Queues are usually replaced with specific Azure services such as Azure Stream Analytics, Azure Queue Storage, Azure Service Bus Queues or RabbitMQ for TD Queues.",
            "Amazon Web Services": "CICS® transient data queue services provide a generalized queueing facility. Data can be queued (stored) for subsequent internal or external processing. You can apply the following functions to selected data, specified in an application:\n\nWrite data to a transient data queue (EXEC CICS WRITEQ TD)\nRead data from a transient data queue (EXEC CICS READQ TD)\nDelete an intrapartition transient data queue (EXEC CICS DELETEQ TD)\n\n\n\n\nAs part of phasing out of the mainframe CICS transaction processing system, the Temporary Storage Queues, Temporary Data Queues are usually replaced with specific AWS services such as Amazon Kinesis Data Analytics, Amazon Simple Queue Service, or RabbitMQ for TD Queues.\n"
        },
        "remediations": {},
        "populations": {
            "Mainframe": ""
        },
        "description": "",
        "impacts": [],
        "effort": null,
        "parents": [
            1202318
        ],
        "outputTable": null,
        "outputGraph": null,
        "children": [
            1202150,
            1202151
        ]
    },
    {
        "name": "CICS Transactions",
        "id": 1202316,
        "category": "BRANCHING-STEP",
        "branchType": null,
        "rationales": {
            "Agnostic": "As part of phasing out of the mainframe CICS transaction processing system in favor of modern web applications, the replacement system should provide Entry points used to start a CICS transaction and run associated programs."
        },
        "remediations": {},
        "populations": {},
        "description": "",
        "impacts": [],
        "effort": null,
        "parents": [
            1202318
        ],
        "outputTable": null,
        "outputGraph": null,
        "children": [
            1202148,
            1202149,
            1202315
        ]
    },
    {
        "name": "CICS Green Screens",
        "id": 1202317,
        "category": "BRANCHING-STEP",
        "branchType": null,
        "rationales": {
            "Agnostic": "Green Screen with multiple access will be complex to review. Each program must be carefully investigated. Please, select one of the programs accessing the green screen and we will check if it is connected to the communication or data access layer."
        },
        "remediations": {},
        "populations": {
            "Mainframe": "search for nodes with type is one of (CICS_MAP, CICS_BMS, CICS_MAPSET, CAST_CICS_MapPrototype, CAST_CICS_MapSetPrototype) "
        },
        "description": "This rule checks any CICS Map, CICS Map definition or CICS Mapset accessed by more than one program ( Monitor link)",
        "impacts": [
            "retire"
        ],
        "effort": "high",
        "parents": [
            1202318
        ],
        "outputTable": null,
        "outputGraph": null,
        "children": [
            1202058,
            1202063,
            1202276
        ]
    },
    {
        "name": "CICS Transactions processing system to replaced with Web application",
        "id": 1202318,
        "category": "BRANCHING-STEP",
        "branchType": null,
        "rationales": {
            "Azure": "Transitioning to Azure necessitates the phasing out of the mainframe CICS transaction processing system in favor of modern web applications.\n\n\nEvaluate the existing CICS application is very important to \n- Identify the scope of the rewrite: which parts of the application need to be rewritten, which can be replaced \n- Determine the appropriate Azure services reproducing CICS services, such as Temporary Storage Queues, Temporary Data Queues or files access: Multiple implementations are usually available, such as Azure Blog Storage, Azure Cosmos DB, File system and Azure Service Bus \n\n\nFor user-facing applications, the BMS screen description format is modernized to an Angular web\n\nNote also that Azure Integration technologies provide multiple mechanisms to interact with CICS systems: Azure Logic Apps support Integration via TCP/IP and HTTP and APPC/\n\n\n ",
            "Amazon Web Services": "Transitioning to AWS necessitates the phasing out of the mainframe CICS transaction processing system in favor of modern web applications.\n\nEvaluate the existing CICS application is very important to \n- Identify the scope of the rewrite: which parts of the application need to be rewritten, which can be replaced \n- Determine the appropriate Aws services reproducing CICS services, such as Temporary Storage Queues, Temporary Data Queues or files access (multiple implementations are usually available, such as Amazon Kinesis Data Analytics, Amazon Simple Queue Service, or RabbitMQ for TD Queues),\n\nFor user-facing applications, the BMS screen description format is modernized to an Angular web"
        },
        "remediations": {},
        "populations": {
            "Mainframe": "search for nodes with type is one of (CICS_MAP, CICS_BMS, CICS_MAPSET, CAST_IMS_MessageFormatService, CAST_IMS_MessageInputDescriptor, CAST_IMS_MessageOutputDescriptor, CAST_CICS_MapPrototype, CAST_CICS_MapSetPrototype) + callees to see which programs linked to these screens"
        },
        "description": "",
        "impacts": [
            "retire"
        ],
        "effort": null,
        "parents": [
            1202319
        ],
        "outputTable": null,
        "outputGraph": null,
        "children": [
            1202314,
            1202316,
            1202317
        ]
    },
    {
        "name": "Mainframe to Azure with Automated Code Refactor",
        "id": 1202319,
        "category": "BRANCHING-STEP",
        "branchType": "CHOICE",
        "rationales": {
            "Azure": "In order to prepare your Mainframe application to be transformed to Azure using Automated Code Refactoring solution (like Advanced..), we will identify dependencies, data access patterns, and other critical elements necessary for the migration process. Based on the analysis, the tools automatically refactor the mainframe code to make it compatible with the cloud environment and Azure services. This may involve converting legacy programming languages or data storage formats to cloud-native equivalents.",
            "Amazon Web Services": "In order to prepare your Mainframe application to be transformed by AWS Automated Code Refactor (like AWS Blue Age), we will identify dependencies, data access patterns, and other critical elements necessary for the migration process. Based on the analysis, the tools automatically refactor the mainframe code to make it compatible with the cloud environment and AWS services. This may involve converting legacy programming languages or data storage formats to cloud-native equivalents."
        },
        "remediations": {},
        "populations": {},
        "description": "",
        "impacts": [],
        "effort": null,
        "parents": [
            1202294
        ],
        "outputTable": null,
        "outputGraph": null,
        "children": [
            1202300,
            1202304,
            1202305,
            1202306,
            1202307,
            1202308,
            1202309,
            1202310,
            1202311,
            1202312,
            1202313,
            1202318
        ]
    },
    {
        "name": "IMS Transactions",
        "id": 1202320,
        "category": "BRANCHING-STEP",
        "branchType": null,
        "rationales": {
            "Agnostic": "Analyzing the existing IMS transactions that you want to expose as web services is a very important step to Understand the input parameters, data formats, and expected responses.\nThis will also help to Determine the approach for transforming IMS transactions into web services: \nOptions include:\nWrap IMS Transactions as Web Services using tools or frameworks to create wrappers around IMS transactions and expose them as web services.\nUse Integration Middleware: Employ middleware solutions like IBM Integration Bus (IIB) or Apache Camel to bridge IMS and web services.\n\nNote also that Existing IMS programs will require code change to handle web service communication"
        },
        "remediations": {},
        "populations": {
            "Mainframe": "CAST_IMS_Transaction + linked programs"
        },
        "description": "",
        "impacts": [
            "rehost"
        ],
        "effort": "low",
        "parents": [
            1202321
        ],
        "outputTable": null,
        "outputGraph": null,
        "children": [
            1202271,
            1202272
        ]
    },
    {
        "name": "IMS Transactions processing system to be Rehosted on Azure Virtual Machines",
        "id": 1202321,
        "category": "BRANCHING-STEP",
        "branchType": null,
        "rationales": {
            "Azure": "Azure VMs can run mainframe emulation environments and compilers that support lift-and-shift scenarios. Common server components that you can emulate include online transaction process (OLTP), batch, and data ingestion systems;\n\nWhen moving IMS-DC application to Azure, emulators for mainframe transaction processing (TP) monitors can run as infrastructure as a service (IaaS) using virtual machines (VMs) on Azure. The web servers can also implement screen handling and form functionality. Combine this approach with database APIs, such as ActiveX Data Objects (ADO), Open Database Connectivity (ODBC), and Java Database Connectivity (JDBC) for data access and transactions.\n\n\nSome specification and configuration options of IMS applications on the Z/OS environment should be reviewed and updated before the integration of emulators for mainframe transaction processing (TP) monitors on Azure VMs\n\nEvaluating the existing IMS application will help you assess the complexity of the migration project, and plan for necessary adjustments that address these specific requirements.\n\n\n",
            "Amazon Web Services": "Emulation solutions such as Microfocus, tmaxsoft, ... used to deploy Mainframe applications on Aws EC2, usually provide the support of IMS DC/DB\n\nSome specification and configuration options should be reviewed and updated before the integration of different IMS components into the new platform. \n\nEvaluating the existing IMS application will help you assess the complexity of the migration project, and plan for necessary adjustments that address these specific requirements.\n"
        },
        "remediations": {},
        "populations": {
            "Mainframe": "search for nodes with type is one of (CICS_MAP, CICS_BMS, CICS_MAPSET, CAST_IMS_MessageFormatService, CAST_IMS_MessageInputDescriptor, CAST_IMS_MessageOutputDescriptor, CAST_CICS_MapPrototype, CAST_CICS_MapSetPrototype) + callees to see which programs linked to these screens"
        },
        "description": "",
        "impacts": [
            "retire"
        ],
        "effort": null,
        "parents": [
            1202295
        ],
        "outputTable": null,
        "outputGraph": null,
        "children": [
            1202261,
            1202320
        ]
    },
    {
        "name": "Avoid using MERGE",
        "id": 5060,
        "category": "TASK",
        "branchType": null,
        "rationales": {
            "Agnostic": "COBOL merges are known to be inefficient on zOS environment. It is better to use external merge."
        },
        "remediations": {
            "Agnostic": "Remove all unnecessary MERGE action and replace them by external tool."
        },
        "populations": null,
        "description": "This rule searches for Cobol programs using the MERGE statement.",
        "impacts": [],
        "effort": "low",
        "parents": [
            1202303
        ],
        "outputTable": null,
        "outputGraph": null,
        "children": []
    },
    {
        "name": "Avoid using ALTER",
        "id": 5062,
        "category": "TASK",
        "branchType": null,
        "rationales": {
            "Agnostic": "The ALTER statement is error-prone. If it is used in Cobol programs, then GO TO statements as they appear in the listing may not be those that will be encountered by the program at run time. The ALTER statement makes the maintenance programmer's job more difficult."
        },
        "remediations": {
            "Agnostic": "If you need to change the processing sequence due to a certain condition, then use an alternative set of PERFORM or GO TO statements."
        },
        "populations": null,
        "description": "This rule searches for Cobol programs using ALTER statements to manage their control flow.",
        "impacts": [],
        "effort": "high",
        "parents": [
            1202302
        ],
        "outputTable": null,
        "outputGraph": null,
        "children": []
    },
    {
        "name": "Avoid DISPLAY ... UPON CONSOLE",
        "id": 5072,
        "category": "TASK",
        "branchType": null,
        "rationales": {
            "Agnostic": "The DISPLAY statement used with the UPON CONSOLE addition sends information to the console and then, it requires operator responses."
        },
        "remediations": {
            "Agnostic": "Only use the DISPLAY statement for debug purpose and do not send information to the console. In the indicator area, use the 'D' character to specify that the statement is for debug version of the program. The DISPLAY statement should only be used to designate the start of the batch program, or the result of the execution of the batch program."
        },
        "populations": null,
        "description": "This rule searches for COBOL programs using the DISPLAY statements with the \"UPON CONSOLE\" addition.",
        "impacts": [],
        "effort": "high",
        "parents": [
            1202302
        ],
        "outputTable": null,
        "outputGraph": null,
        "children": []
    },
    {
        "name": "Avoid using GOTO statement (COBOL)",
        "id": 5144,
        "category": "TASK",
        "branchType": null,
        "rationales": {
            "Agnostic": "Using GOTO code breaks the process execution flow and makes the code more difficult to understand and maintain."
        },
        "remediations": {
            "Agnostic": "Try to restructure the program and replace GOTO jumps with PERFORM."
        },
        "populations": null,
        "description": "This rule searches for Cobol programs using GOTO statements to manage the control flow. The \"GOTO\" statement of embedded SQL is not taken into account.",
        "impacts": [],
        "effort": "low",
        "parents": [
            1202303
        ],
        "outputTable": null,
        "outputGraph": null,
        "children": []
    },
    {
        "name": "Avoid using PREPARE STMT statement (Dynamic SQL) with STRING containing HOST variables",
        "id": 8480,
        "category": "TASK",
        "branchType": null,
        "rationales": {
            "Agnostic": "Injection flaws, such as SQL, NoSQL, OS, and LDAP injection, occur when untrusted data is sent to an interpreter as part of a command or query. The attacker's hostile data can trick the interpreter into executing unintended commands or accessing data without proper authorization."
        },
        "remediations": {
            "Agnostic": "The PREPARE STMT statement having STRING as a parameter must not contain host variables."
        },
        "populations": null,
        "description": "Input host variables are used to specify data to be transferred from the COBOL program to the database\n\nHost variables are considered data, not part of the SQL code, which is safe against SQL injection attacks. But dynamic SQL (PREPARE or EXECUTE IMMEDIATE) is allowed, and binding does not prevent the injection of code.\n\nThis rule checks for PREPARE STMT statement having STRING passed in parameter and containing host variables.",
        "impacts": [],
        "effort": "high",
        "parents": [
            1202302
        ],
        "outputTable": null,
        "outputGraph": null,
        "children": []
    },
    {
        "name": "Ensure MERGE statement is not used",
        "id": 1106002,
        "category": "TASK",
        "branchType": null,
        "rationales": {
            "Agnostic": "MERGE statement is not supported and it cannot be automatically converted by AWS SCT. Manual conversion is straight-forward in most cases."
        },
        "remediations": {
            "Agnostic": "PostgreSQL does not support the usage of MERGE SQL statement. \nAs an alternative, consider using the INSERT… ON CONFLICT clause, which can handle cases where insert clauses might cause a conflict, and then redirect the operation as an update."
        },
        "populations": null,
        "description": "This rule checks if Oracle MERGE Statement is used.",
        "impacts": [],
        "effort": "low",
        "parents": [
            1202293
        ],
        "outputTable": null,
        "outputGraph": null,
        "children": []
    },
    {
        "name": "Ensure PL/SQL package UTL_FILE is not used",
        "id": 1106006,
        "category": "TASK",
        "branchType": null,
        "rationales": {
            "Agnostic": "PostgreSQL doesn’t currently provide a directly comparable alternative for the Oracle UTL_FILE package."
        },
        "remediations": {
            "Agnostic": "PostgreSQL doesn’t currently provide a directly comparable alternative for the Oracle UTL_FILE package."
        },
        "populations": null,
        "description": "This rule checks the usage of the UTL_FILE package in the Oracle Database(s).",
        "impacts": [],
        "effort": "high",
        "parents": [
            1202292
        ],
        "outputTable": null,
        "outputGraph": null,
        "children": []
    },
    {
        "name": "Ensure PL/SQL package DBMS_SCHEDULER is not used",
        "id": 1106008,
        "category": "TASK",
        "branchType": null,
        "rationales": {
            "Agnostic": "DBMS_SCHEDULER is not supported by Aurora PostgreSQL"
        },
        "remediations": {
            "Agnostic": "PostgreSQL doesn’t currently provide a directly comparable alternative for the Oracle DBMS_SCHEDULER package."
        },
        "populations": null,
        "description": "This rule checks the usage of the DBMS_SCHEDULER package in the Oracle Database(s).",
        "impacts": [],
        "effort": "high",
        "parents": [
            1202292
        ],
        "outputTable": null,
        "outputGraph": null,
        "children": []
    },
    {
        "name": "Ensure PL/SQL package DBMS_RANDOM is not used",
        "id": 1106010,
        "category": "TASK",
        "branchType": null,
        "rationales": {
            "Agnostic": "Oracle’s DBMS_RANDOM package provides functionality for generating random numbers or strings as part of an SQL statement or PL/SQL procedure. PostgreSQL does not provide a dedicated package equivalent to Oracle DBMS_RANDOM : Migration is not possible. However, other PostgreSQL functions can be used as workarounds under certain conditions."
        },
        "remediations": {
            "Agnostic": "PostgreSQL: Generating random numbers can be performed using the random() function. \nFor generating random strings, you can use the value returned from the random() function coupled with an md5() function."
        },
        "populations": null,
        "description": "This rule checks the usage of the DBMS_RANDOM package in the Oracle Database(s).",
        "impacts": [],
        "effort": "high",
        "parents": [
            1202292
        ],
        "outputTable": null,
        "outputGraph": null,
        "children": []
    },
    {
        "name": "Ensure PL/SQL package DBMS_OUTPUT is not used",
        "id": 1106012,
        "category": "TASK",
        "branchType": null,
        "rationales": {
            "Agnostic": "The Oracle DBMS_OUTPUT package is typically used for debugging or for displaying output messages from PL/SQL procedures.\n\nAmazon Aurora PostgreSQL doesn’t currently provides a directly comparable alternative for Oracle DBMS_OUTPUT package."
        },
        "remediations": {
            "Agnostic": "You can use the PostgreSQL RAISE statement as an alternative to DBMS_OUTPUT.\n\nHave a look to github \"orafce\" project, maybe you can find a better remediation."
        },
        "populations": null,
        "description": "This rule checks the usage of the DBMS_OUTPUT package in Oracle Database(s).",
        "impacts": [],
        "effort": "high",
        "parents": [
            1202292
        ],
        "outputTable": null,
        "outputGraph": null,
        "children": []
    },
    {
        "name": "Ensure PL/SQL package DBMS_AUTO_INDEX is not used",
        "id": 1106014,
        "category": "TASK",
        "branchType": null,
        "rationales": {
            "Agnostic": "Package DBMS_AUTO_INDEX is used to configure auto indexes and generate reports.\nPostgreSQL does not provide an Automatic Indexing feature.\n\nIn self-managed PostgreSQL instances, extensions like Dexter (https://github.com/ankane/dexter) and HypoPG (https://hypopg.readthedocs.io/en/latest/ ) can be utilized for generating indexes with limitations. Amazon Aurora PostgreSQL does not support these extensions."
        },
        "remediations": {
            "Agnostic": "Remove References to DBMS_AUTO_INDEX and replace with PostgreSQL Solution.\n\nPostgreSQL does not provide Automatic Indexing feature, but in self-managed PostgreSQL instances, extensions like Dexter (https://github.com/ankane/dexter) and HypoPG (https://hypopg.readthedocs.io/en/latest/ ) can\nbe utilized for generating indexes with limitations (Amazon Aurora PostgreSQL does not support these extensions).\nThe approach taken by these extensions are :\n- Identify the queries.\n- Update the table statistics if they haven’t been analyzed recently.\n- Get the initial cost of the queries and create hypothetical indexes on columns that aren’t already indexes.\n- Get costs again and see if any hypothetical indexes were used. Hypothetical indexes that were used and significantly reduced cost are selected to be indexes."
        },
        "populations": null,
        "description": "This rule checks the usage or DBMS_AUTO_INDEX package in Oracle Database(s).",
        "impacts": [],
        "effort": "high",
        "parents": [
            1202292
        ],
        "outputTable": null,
        "outputGraph": null,
        "children": []
    },
    {
        "name": "Ensure LOB data types are not used",
        "id": 1106020,
        "category": "TASK",
        "branchType": null,
        "rationales": {
            "Agnostic": "LOB storage is not supported by PostgreSQL."
        },
        "remediations": {
            "Agnostic": "Replace Oracle LOB data types with the following PostgreSQL data types:\n- BFILE can be replaced with VARCHAR(255) or CHARACTER VARYING(255)\n- BLOB can be replaced with BYTEA\n- CLOB and NCLOB can be replaced with TEXT"
        },
        "populations": null,
        "description": "This rule checks the usage of LOB Storage in Oracle Database(s) and the explicit conversion functions TO_CLOB and TO_NCLOB which convert other data types to LOB data types.",
        "impacts": [],
        "effort": "moderate",
        "parents": [
            1202291
        ],
        "outputTable": null,
        "outputGraph": null,
        "children": []
    },
    {
        "name": "Ensure you don't have unsupported option GENERATED BY DEFAULT for identity columns",
        "id": 1106022,
        "category": "TASK",
        "branchType": null,
        "rationales": {
            "Agnostic": "Identity Columns with GENERATED BY DEFAULT option are not supported by PostgreSQL. But PostgreSQL suggests a specific way to create an autoincrementing column as equivalent feature"
        },
        "remediations": {
            "Agnostic": "PostgreSQL enables you to create a sequence that is similar to the IDENTITY property supported by Oracle 12c identity column feature. When creating a new table using the SERIAL pseudo-type, a sequence is created.\n\nAdditional types from the same family are SMALLSERIAL and BIGSERIAL.\nBy assigning a SERIAL type to a column as part of table creation, PostgreSQL creates a sequence using default configuration and adds the NOT NULL constraint to the column. The new sequence can be altered and configured as a regular sequence.\n\nSince PostgreSQL 10, there is a new option called identity columns which is similar to SERIAL data type but more SQL standard compliant. The identity columns are highly compatibility compare to Oracle identity columns."
        },
        "populations": null,
        "description": "This rule checks if tables with identity columns has GENERATED BY DEFAULT option in Oracle Database(s).",
        "impacts": [],
        "effort": "low",
        "parents": [
            1202293
        ],
        "outputTable": null,
        "outputGraph": null,
        "children": []
    },
    {
        "name": "Ensure Database Links are not used",
        "id": 1106036,
        "category": "TASK",
        "branchType": null,
        "rationales": {
            "Agnostic": "Migrating database links from Oracle to PostgreSQL requires a full rewrite the mechanism that managed the database links."
        },
        "remediations": {
            "Agnostic": "PostgreSQL Usage\nQuerying data in remote databases in PostgreSQL is available via two primary options:\n1. dblink database link function.\n2. postgresql_fdw (Foreign Data Wrapper, FDW) extension.\n\nThe Postgres foreign data wrapper extension is new to PostgreSQL and offers functionality that is similar to dblink. However, the Postgres foreign data wrapper aligns closer with the SQL standard and can provide improved performance."
        },
        "populations": null,
        "description": "This rule checks if database links objects exists or are referenced in Oracle Database(s).",
        "impacts": [],
        "effort": "high",
        "parents": [
            1202292
        ],
        "outputTable": null,
        "outputGraph": null,
        "children": []
    },
    {
        "name": "Ensure Unused Column is not used",
        "id": 1106038,
        "category": "TASK",
        "branchType": null,
        "rationales": {
            "Agnostic": "PostgreSQL doesn’t support marking table columns as unused."
        },
        "remediations": {
            "Agnostic": "PostgreSQL doesn’t support marking table columns as unused. However, when running the ALTER TABLE… DROP COLUMN command, the drop column statement doesn’t physically remove the column; it only makes it invisible to SQL operations"
        },
        "populations": null,
        "description": "This rule checks if any Column table is set as Unused in the Oracle Database(s).",
        "impacts": [],
        "effort": "low",
        "parents": [
            1202293
        ],
        "outputTable": null,
        "outputGraph": null,
        "children": []
    },
    {
        "name": "Ensure READ ONLY Table is not used",
        "id": 1106040,
        "category": "TASK",
        "branchType": null,
        "rationales": {
            "Agnostic": "READ ONLY mode for Table is not supported by PostgreSQL."
        },
        "remediations": {
            "Agnostic": "PostgreSQL does not provide an equivalent to the READ ONLY mode supported in Oracle.\nThe following alternatives could be used as workarounds:\nl “Read-only” User or Role.\nl “Read-only” database.\nl Creating a “read-only” database trigger or a using a “read-only” constraint."
        },
        "populations": null,
        "description": "This rule checks if any READ ONLY table exists in Oracle Database(s).",
        "impacts": [],
        "effort": "high",
        "parents": [
            1202292
        ],
        "outputTable": null,
        "outputGraph": null,
        "children": []
    },
    {
        "name": "Ensure Invisible Index is not used",
        "id": 1106042,
        "category": "TASK",
        "branchType": null,
        "rationales": {
            "Agnostic": "PostgreSQL does not support Invisible Indexes"
        },
        "remediations": {
            "Agnostic": "Currently, PostgreSQL does not provide a directly comparable alternative for Oracle Invisible Indexes."
        },
        "populations": null,
        "description": "This rule checks if any Invisible Indexes exists in Oracle Database(s).",
        "impacts": [],
        "effort": "high",
        "parents": [
            1202292
        ],
        "outputTable": null,
        "outputGraph": null,
        "children": []
    },
    {
        "name": "Ensure PL/SQL packages UTL_MAIL and UTL_SMTP are not used",
        "id": 1106044,
        "category": "TASK",
        "branchType": null,
        "rationales": {
            "Agnostic": "The Oracle UTL_MAIL package provides functionality for sending email messages. Unlike UTL_SMTP, which is more complex and provided in earlier versions of Oracle, UTL_MAIL supports attachment."
        },
        "remediations": {
            "Agnostic": "The remediation is to remove all references to UTL_MAIL or UTL_SMTP and try to replace with a PostgreSQL solution."
        },
        "populations": null,
        "description": "This rule checks if the UTL_MAIL or UTL_SMTP PL/SQL packages are used in Oracle Database(s).",
        "impacts": [],
        "effort": "high",
        "parents": [
            1202292
        ],
        "outputTable": null,
        "outputGraph": null,
        "children": []
    },
    {
        "name": "Ensure BLOB data type is not used",
        "id": 1106052,
        "category": "TASK",
        "branchType": null,
        "rationales": {
            "Agnostic": "As you decide to migrate from DB2 to PostgreSQL, it is important to note the changes to be taken care of during the migration process. Without adaptation, the SQL script won’t compile on Postgresql, leading to error"
        },
        "remediations": {
            "Agnostic": "Replace BLOB(n) data type with BYTEA data type."
        },
        "populations": null,
        "description": "This rule checks if any BLOB data type is used in DB2 Database(s).",
        "impacts": [],
        "effort": "moderate",
        "parents": [
            1202286
        ],
        "outputTable": null,
        "outputGraph": null,
        "children": []
    },
    {
        "name": "Ensure CLOB data type is not used",
        "id": 1106054,
        "category": "TASK",
        "branchType": null,
        "rationales": {
            "Agnostic": "As you decide to migrate from DB2 to PostgreSQL, it is important to note the changes to be taken care of during the migration process. Without adaptation, the SQL script won’t compile on Postgresql, leading to error"
        },
        "remediations": {
            "Agnostic": "Replace CLOB(n) data type with TEXT data type."
        },
        "populations": null,
        "description": "This rule checks the usage of CLOB Storage and the explicit conversion function TO_CLOB which convert other data types to CLOB.",
        "impacts": [],
        "effort": "moderate",
        "parents": [
            1202286
        ],
        "outputTable": null,
        "outputGraph": null,
        "children": []
    },
    {
        "name": "Ensure VARCHAR(n) FOR BIT DATA data type is not used",
        "id": 1106058,
        "category": "TASK",
        "branchType": null,
        "rationales": {
            "Agnostic": "As you decide to migrate from DB2 to PostgreSQL, it is important to note the changes to be taken care of during the migration process. Without adaptation, the SQL script won’t compile on Postgresql, leading to error"
        },
        "remediations": {
            "Agnostic": "Replace VARCHAR(n) FOR BIT DATA data type with BYTEA data type."
        },
        "populations": null,
        "description": "This rule checks if any VARCHAR(n) FOR BIT DATA data type is used in DB2 Database(s).",
        "impacts": [],
        "effort": "moderate",
        "parents": [
            1202286
        ],
        "outputTable": null,
        "outputGraph": null,
        "children": []
    },
    {
        "name": "Ensure NCHAR VARYING data type is not used",
        "id": 1106064,
        "category": "TASK",
        "branchType": null,
        "rationales": {
            "Agnostic": "As you decide to migrate from DB2 to PostgreSQL, it is important to note the changes to be taken care of during the migration process. Without adaptation, the SQL script won’t compile on Postgresql, leading to error"
        },
        "remediations": {
            "Agnostic": "Replace NCHAR VARYING(n) data type with VARCHAR(n) data type"
        },
        "populations": null,
        "description": "This rule checks if any NCHAR VARYING data type is used in DB2 Database(s).",
        "impacts": [],
        "effort": "moderate",
        "parents": [
            1202286
        ],
        "outputTable": null,
        "outputGraph": null,
        "children": []
    },
    {
        "name": "Ensure GRAPHIC data type is not used",
        "id": 1106066,
        "category": "TASK",
        "branchType": null,
        "rationales": {
            "Agnostic": "As you decide to migrate from DB2 to PostgreSQL, it is important to note the changes to be taken care of during the migration process. Without adaptation, the SQL script won’t compile on Postgresql, leading to error"
        },
        "remediations": {
            "Agnostic": "Replace GRAPHIC(n) data type with CHAR(n) data type"
        },
        "populations": null,
        "description": "This rule checks if any GRAPHIC data type is used in DB2 Database(s).",
        "impacts": [],
        "effort": "moderate",
        "parents": [
            1202286
        ],
        "outputTable": null,
        "outputGraph": null,
        "children": []
    },
    {
        "name": "Ensure FLOAT data type is not used",
        "id": 1106068,
        "category": "TASK",
        "branchType": null,
        "rationales": {
            "Agnostic": "As you decide to migrate from DB2 to PostgreSQL, it is important to note the changes to be taken care of during the migration process. Without adaptation, the SQL script won’t compile on Postgresql, leading to error."
        },
        "remediations": {
            "Agnostic": "Replace FLOAT(p) data type with DOUBLE PRECISION data type."
        },
        "populations": null,
        "description": "This rule checks if any FLOAT data type is used in DB2 Database(s).",
        "impacts": [],
        "effort": "moderate",
        "parents": [
            1202286
        ],
        "outputTable": null,
        "outputGraph": null,
        "children": []
    },
    {
        "name": "Ensure DECFLOAT data type is not used",
        "id": 1106070,
        "category": "TASK",
        "branchType": null,
        "rationales": {
            "Agnostic": "As you decide to migrate from DB2 to PostgreSQL, it is important to note the changes to be taken care of during the migration process. Without adaptation, the SQL script won’t compile on Postgresql, leading to error"
        },
        "remediations": {
            "Agnostic": "Replace DECFLOAT(16|34) data type with FLOAT data type."
        },
        "populations": null,
        "description": "This rule checks if any DECFLOAT data type is used in DB2 Database(s).",
        "impacts": [],
        "effort": "moderate",
        "parents": [
            1202286
        ],
        "outputTable": null,
        "outputGraph": null,
        "children": []
    },
    {
        "name": "Ensure DB2 Scalar Function DATE is not used",
        "id": 1106072,
        "category": "TASK",
        "branchType": null,
        "rationales": {
            "Agnostic": "As you decide to migrate from DB2 to PostgreSQL, it is important to note the changes to be taken care of during the migration process. Without adaptation, the SQL script won’t compile on Postgresql, leading to error"
        },
        "remediations": {
            "Agnostic": "Converts the input to date value :\nDATE('2006-09-21') in DB2 to be changed to TO_DATE('21-02-2006','DD-MMYYYY') in PostgreSQL."
        },
        "populations": null,
        "description": "This rule checks the DB2 Scalar Function DATE is used in DB2 Database(s).",
        "impacts": [],
        "effort": "low",
        "parents": [
            1202288
        ],
        "outputTable": null,
        "outputGraph": null,
        "children": []
    },
    {
        "name": "Ensure DB2 Scalar Function SMALLINT is not used",
        "id": 1106074,
        "category": "TASK",
        "branchType": null,
        "rationales": {
            "Agnostic": "As you decide to migrate from DB2 to PostgreSQL, it is important to note the changes to be taken care of during the migration process. Without adaptation, the SQL script won’t compile on Postgresql, leading to error."
        },
        "remediations": {
            "Agnostic": "In DB2, SMALLINT converts either a number or a valid character value into a smallint value.SMALLINT(<Decimal Number>) in DB2 gets replaced by TO_NUMBER(<field>, <format>) in PostgreSQL."
        },
        "populations": null,
        "description": "This rule checks the DB2 Scalar Function SMALLINT is used in DB2 Database(s).",
        "impacts": [],
        "effort": "low",
        "parents": [
            1202288
        ],
        "outputTable": null,
        "outputGraph": null,
        "children": []
    },
    {
        "name": "Ensure DB2 Scalar Function LCASE is not used",
        "id": 1106076,
        "category": "TASK",
        "branchType": null,
        "rationales": {
            "Agnostic": "As you decide to migrate from DB2 to PostgreSQL, it is important to note the changes to be taken care of during the migration process. Without adaptation, the SQL script won’t compile on Postgresql, leading to error."
        },
        "remediations": {
            "Agnostic": "LOWER(<FIELD>) (or) LCASE(<FIELD>) is supported in DB2 where as only LOWER(<FIELD>) is supported in PostgreSQL. Thus, if LCASE(<FIELD>) is used in DB2, then it should be changed to LOWER(<FIELD>) for PostgreSQL."
        },
        "populations": null,
        "description": "This rule checks the DB2 Scalar Function LCASE is used in DB2 Database(s).",
        "impacts": [],
        "effort": "low",
        "parents": [
            1202288
        ],
        "outputTable": null,
        "outputGraph": null,
        "children": []
    },
    {
        "name": "Ensure DB2 Scalar Function POSSTR is not used",
        "id": 1106078,
        "category": "TASK",
        "branchType": null,
        "rationales": {
            "Agnostic": "As you decide to migrate from DB2 to PostgreSQL, it is important to note the changes to be taken care of during the migration process. Without adaptation, the SQL script won’t compile on Postgresql, leading to error."
        },
        "remediations": {
            "Agnostic": "POSSTR(<FIELD_1>, <FIELD_2>) in DB2 gets replaced by  POSITION(<FIELD_1> IN <FIELD_2>) in PostgreSQL."
        },
        "populations": null,
        "description": "This rule checks the DB2 Scalar Function POSSTR is used in DB2 Database(s).",
        "impacts": [],
        "effort": "low",
        "parents": [
            1202288
        ],
        "outputTable": null,
        "outputGraph": null,
        "children": []
    },
    {
        "name": "Ensure DB2 Scalar Function RAND is not used",
        "id": 1106080,
        "category": "TASK",
        "branchType": null,
        "rationales": {
            "Agnostic": "As you decide to migrate from DB2 to PostgreSQL, it is important to note the changes to be taken care of during the migration process. Without adaptation, the SQL script won’t compile on Postgresql, leading to error."
        },
        "remediations": {
            "Agnostic": "RAND() in DB2 get replaced by RANDOM() in PostgreSQL and Returns a pseudorandom floating-point value in the range of zero to one inclusive."
        },
        "populations": null,
        "description": "This rule checks the DB2 Scalar Function RAND is used in DB2 Database(s).",
        "impacts": [],
        "effort": "low",
        "parents": [
            1202288
        ],
        "outputTable": null,
        "outputGraph": null,
        "children": []
    },
    {
        "name": "Ensure DB2 Scalar Function TIMESTAMP is not used",
        "id": 1106082,
        "category": "TASK",
        "branchType": null,
        "rationales": {
            "Agnostic": "As you decide to migrate from DB2 to PostgreSQL, it is important to note the changes to be taken care of during the migration process. Without adaptation, the SQL script won’t compile on Postgresql, leading to error."
        },
        "remediations": {
            "Agnostic": "TIMESTAMP(<FIELD>) Converts the input into a time value, and it has to be changed to TO_TIMESTAMP (<FIELD>, <format>) in PostgreSQL.\nCURRENT TIMESTAMP has to be changed to CURRENT_TIMESTAMP."
        },
        "populations": null,
        "description": "This rule checks the DB2 Scalar Function TIMESTAMP is used in DB2 Database(s).",
        "impacts": [],
        "effort": "low",
        "parents": [
            1202288
        ],
        "outputTable": null,
        "outputGraph": null,
        "children": []
    },
    {
        "name": "Ensure DB2 Scalar Function CHAR is not used",
        "id": 1106084,
        "category": "TASK",
        "branchType": null,
        "rationales": {
            "Agnostic": "As you decide to migrate from DB2 to PostgreSQL, it is important to note the changes to be taken care of during the migration process. Without adaptation, the SQL script won’t compile on Postgresql, leading to error"
        },
        "remediations": {
            "Agnostic": "CHAR in DB2 should be replaced by TO_CHAR( <timestamp / interval / int / double precision / numeric type>, text) in PostgreSQL."
        },
        "populations": null,
        "description": "This rule checks the DB2 Scalar Function CHAR is used in DB2 Database(s).",
        "impacts": [],
        "effort": "low",
        "parents": [
            1202288
        ],
        "outputTable": null,
        "outputGraph": null,
        "children": []
    },
    {
        "name": "Ensure DB2 Scalar Function MONTH is not used",
        "id": 1106090,
        "category": "TASK",
        "branchType": null,
        "rationales": {
            "Agnostic": "As you decide to migrate from DB2 to PostgreSQL, it is important to note the changes to be taken care of during the migration process. Without adaptation, the SQL script won’t compile on Postgresql, leading to error"
        },
        "remediations": {
            "Agnostic": "Returns the month part of the date value. The output format is integer.\nReplace DATE_PART ('MONTH', <DATE_FIELD>) with MONTH (<DATE_FIELD>)."
        },
        "populations": null,
        "description": "This rule checks the DB2 Scalar Function MONTH is used in DB2 Database(s).",
        "impacts": [],
        "effort": "low",
        "parents": [
            1202288,
            1202288
        ],
        "outputTable": null,
        "outputGraph": null,
        "children": []
    },
    {
        "name": "Ensure DB2 Scalar Functions DAY/DAYS are not used",
        "id": 1106092,
        "category": "TASK",
        "branchType": null,
        "rationales": {
            "Agnostic": "As you decide to migrate from DB2 to PostgreSQL, it is important to note the changes to be taken care of during the migration process. Without adaptation, the SQL script won’t compile on Postgresql, leading to error"
        },
        "remediations": {
            "Agnostic": "DAY returns the day (as in day of the month) part of a date (or equivalent) value. The output format is integer.\nDAY (<DATE_FIELD>) should be replaced with : DATE_PART(‘day’, <DATE_FIELD>).\n\nDAYS converts a date (or equivalent) value into a number that represents the number of days since the date \"0001-01-01\" inclusive. The output format is integer.\nDAYS is not available in PostgreSQL."
        },
        "populations": null,
        "description": "This rule checks the DB2 Scalar Functions DAY/DAYS are used in DB2 Database(s).",
        "impacts": [],
        "effort": "low",
        "parents": [
            1202288
        ],
        "outputTable": null,
        "outputGraph": null,
        "children": []
    },
    {
        "name": "Ensure DB2 Scalar Functions DECIMAL/DEC are not used",
        "id": 1106094,
        "category": "TASK",
        "branchType": null,
        "rationales": {
            "Agnostic": "As you decide to migrate from DB2 to PostgreSQL, it is important to note the changes to be taken care of during the migration process. Without adaptation, the SQL script won’t compile on Postgresql, leading to error"
        },
        "remediations": {
            "Agnostic": "DECIMAL/DEC converts either character or numeric input to decimal. PostgreSQL has no direct equivalent, you should use TO_NUMBER instead."
        },
        "populations": null,
        "description": "This rule checks the DB2 Scalar Functions DECIMAL/DEC are used in DB2 Database(s).",
        "impacts": [],
        "effort": "low",
        "parents": [
            1202288
        ],
        "outputTable": null,
        "outputGraph": null,
        "children": []
    },
    {
        "name": "Ensure PARTITION is not used in ALTER TABLE statements",
        "id": 1106096,
        "category": "TASK",
        "branchType": null,
        "rationales": {
            "Agnostic": "AWS SCT can convert Db2 LUW tables to partitioned tables in PostgreSQL 10. There are several restrictions when converting a Db2 LUW partitioned table to PostgreSQL:\n\nYou can create a partitioned table with a nullable column in Db2 LUW, and you can specify a partition to store NULL values. However, PostgreSQL doesn’t support NULL values for RANGE partitioning.\n\nDb2 LUW can use an INCLUSIVE or EXCLUSIVE clause to set range boundary values. PostgreSQL only supports INCLUSIVE for a starting boundary and EXCLUSIVE for an ending boundary. The converted partition name is in the format <original_table_name>_<original_partition_name>.\n\nYou can create primary or unique keys for partitioned tables in Db2 LUW. PostgreSQL requires you to create primary or unique key for each partition directly. Primary or unique key constraints must be removed from the parent table. The converted key name is in the format <original_key_name>_<original_partition _name>.\n\nYou can create a foreign key constraint from and to a partitioned table in Db2 LUW. However, PostgreSQL doesn’t support foreign keys references in partitioned tables. PostgreSQL also doesn’t support foreign key references from a partitioned table to another table.\n\nYou can create an index on a partitioned table in Db2 LUW. However, PostgreSQL requires you to create an index for each partition directly. Indexes must be removed from the parent table. The converted index name is in the format <original_index_name>_<original_partition_name>.\n\nYou must define row triggers on individual partitions, not on the partitioned table. Triggers must be removed from the parent table. The converted trigger name is in the format <original_trigger_name>_<original_partition_name>."
        },
        "remediations": {
            "Agnostic": "The remediation is to remove unsupported options."
        },
        "populations": null,
        "description": "This rule will check if the following sql statements exist into DB2 SQL files:\n   ALTER TABLE ADD PARTITION\n   ALTER TABLE DETACH PARTITION\n   ALTER TABLE ATTACH PARTITION",
        "impacts": [],
        "effort": "low",
        "parents": [
            1202288
        ],
        "outputTable": null,
        "outputGraph": null,
        "children": []
    },
    {
        "name": "Ensure DECLARE cursor is not used",
        "id": 1106100,
        "category": "TASK",
        "branchType": null,
        "rationales": {
            "Agnostic": "As you decide to migrate from DB2 to PostgreSQL, it is important to note the changes to be taken care of during the migration process. Without adaptation, the SQL script won’t compile on Postgresql, leading to error"
        },
        "remediations": {
            "Agnostic": "Cursor operations SQL statements used in stored procedures, functions and triggers from IBM DB2 to PostgreSQL should be Converted as follows: \n\n1-Cursor declaration: DECLARE cur CURSOR FOR query  To be Replaced by  cur CURSOR FOR query\n2-Return result set DECLARE cur CURSOR WITH RETURN  To be Replaced by cur REFCURSOR\n3-Dynamic cursors: DECLARE cur WITH RETURN FOR stmt PREPARE stmt FROM 'query_string'  To be Replaced by  OPEN cur FOR EXECUTE 'query_string'"
        },
        "populations": null,
        "description": "This rule checks Cursor operations 'DECLARE cur' Statements in SQL DB2 files and Embedded SQL Queries.",
        "impacts": [],
        "effort": "low",
        "parents": [
            1202288
        ],
        "outputTable": null,
        "outputGraph": null,
        "children": []
    },
    {
        "name": "Ensure DB2 DECLARE variable is not used",
        "id": 1106102,
        "category": "TASK",
        "branchType": null,
        "rationales": {
            "Agnostic": "As you decide to migrate from DB2 to PostgreSQL, it is important to note the changes to be taken care of during the migration process. Without adaptation, the SQL script won’t compile on Postgresql, leading to error."
        },
        "remediations": {
            "Agnostic": "Variable declarations statements used in DML files, stored procedures, functions and triggers should be converted before re-platforming your DB2 Database to PostgreSQL:\n\n1- Declarations are inside BEGIN END block   Should be before BEGIN END block\n2- DECLARE var datatype DEFAULT value        Should be replaced by var datatype DEFAULT value\n3- DECLARE var, var2, … datatype   Should be replaced by  var datatype; var2 datatype; …"
        },
        "populations": null,
        "description": "This rule checks the variable declaration in DB2 Database(s).",
        "impacts": [],
        "effort": "low",
        "parents": [
            1202288
        ],
        "outputTable": null,
        "outputGraph": null,
        "children": []
    },
    {
        "name": "Ensure SYSIBM.SYSDUMMY1 table is not used",
        "id": 1106104,
        "category": "TASK",
        "branchType": null,
        "rationales": {
            "Agnostic": "As you decide to migrate from DB2 to PostgreSQL, it is important to note the changes to be taken care of during the migration process. Without adaptation, the SQL script won’t compile on Postgresql, leading to error"
        },
        "remediations": {
            "Agnostic": "There is no “SYSIBM.SYSDUMMY1” table in PostgreSQL. PostgreSQL allows a “SELECT” without ”FROM” clause. You can remove this by using script."
        },
        "populations": null,
        "description": "This rule checks if SYSIBM.SYSDUMMY1 table is used in DB2 Database(s).",
        "impacts": [],
        "effort": "high",
        "parents": [
            1202287
        ],
        "outputTable": null,
        "outputGraph": null,
        "children": []
    },
    {
        "name": "Ensure VALUES statement is not used",
        "id": 1106106,
        "category": "TASK",
        "branchType": null,
        "rationales": {
            "Agnostic": "As you decide to migrate from DB2 to PostgreSQL, it is important to note the changes to be taken care of during the migration process. Without adaptation, the SQL script won’t compile on Postgresql, leading to error."
        },
        "remediations": {
            "Agnostic": "DB2 VALUES statement should be replaced in PostgresSQL with SELECT ... UNION ALL SELECT ..."
        },
        "populations": null,
        "description": "This rule checks the usage of the VALUES statement in the DB2 Database(s).",
        "impacts": [],
        "effort": "low",
        "parents": [
            1202288
        ],
        "outputTable": null,
        "outputGraph": null,
        "children": []
    },
    {
        "name": "Ensure DB2 SET variable is not used",
        "id": 1106108,
        "category": "TASK",
        "branchType": null,
        "rationales": {
            "Agnostic": "As you decide to migrate from DB2 to PostgreSQL, it is important to note the changes to be taken care of during the migration process. Without adaptation, the SQL script won’t compile on Postgresql, leading to error."
        },
        "remediations": {
            "Agnostic": "Converting Variable setting statements used in stored procedures, functions and triggers from IBM DB2 to PostgreSQL as follows: \n\n1    SET v1 = value   To be replaced by   v1 := value\n2    SET v1 = value, v2 = value2, …   To be replaced by   v1 := value; v2 := value2; …\n3    SET (v1, v2, …) = (value, value2, …)   To be replaced by   v1 := value; v2 := value2; …\n4    SET (v1, v2, …) = (SELECT c1, c2, …)   To be replaced by   SELECT c1, c2, … INTO v1, v2, …"
        },
        "populations": null,
        "description": "This rule checks the variable assignment in DB2 Database(s).",
        "impacts": [],
        "effort": "low",
        "parents": [
            1202288
        ],
        "outputTable": null,
        "outputGraph": null,
        "children": []
    },
    {
        "name": "Cobol Programs candidate for Cloud function conversion: Data-dependent Programs",
        "id": 1202001,
        "category": "TASK",
        "branchType": null,
        "rationales": {
            "Agnostic": "Check all programs accessing Database Objects\n\nData-dependent programs accessing SHARED data file/DB table"
        },
        "remediations": {},
        "populations": {
            "Mainframe": "CAST_COBOL_SavedProgram with the property 'Number of Transaction' = 1\nThe Entry Point of the transaction should have the property 'Number of Transaction' = 1\n\n\n\nThe program is accessing a SHARED Database table or SHARED DataSet File Storage\nA SHARED Database table or SHARED DataSet File Storage: means that the table or NOT the DataSet File Storage is accessed MORE THEN the Program  \n "
        },
        "description": "Among the candidates for AWS Lambda function conversion, This rule identifies the program accessing the SHARED Database table or Any DataSet File Storage\n\nChange will be required before conversion",
        "impacts": [],
        "effort": "high",
        "parents": [
            1202300
        ],
        "outputTable": {
            "template": "none",
            "table": {
                "head": [
                    "COBOL PROGRAM FULLNAME",
                    "TRANSACTION NAME"
                ],
                "body": null
            }
        },
        "outputGraph": {
            "template": "none"
        },
        "children": []
    },
    {
        "name": "PDS Files: Programs/Utilities manipulating Data",
        "id": 1202014,
        "category": "TASK",
        "branchType": null,
        "rationales": {
            "Agnostic": "Please review each Cobol Program reading/writing in PDS Dataset in order to plan replacement using SQL statements."
        },
        "remediations": {},
        "populations": {
            "Mainframe": "(EXISTS {MATCH (o:Object)-[:Property {value:'PDS Dataset'}]->(op:ObjectProperty)WHERE op.Id IN ['141023']})\n\nFor each PDS dataset called by Cobol Program: PDS dataset + Calling Program + ACCESS MODE\nFor each PDS dataset called by Any Utility: PDS dataset + Calling Any Utility + ACCESS MODE\n"
        },
        "description": "For Each used PDS Dataset, we display the access mode(s) (Read/Write mode) and Calling Cobol Program or Utility",
        "impacts": [],
        "effort": "low",
        "parents": [
            1202305
        ],
        "outputTable": {
            "template": "inbound-calls-table",
            "table": {
                "head": [
                    "PDS FILE",
                    "Cobol Program/Paragraph",
                    "ACCESS WITH TYPE"
                ],
                "body": [
                    "PDS File FullNAME",
                    "CALLER Cobol Program/Paragraph FullNAME",
                    "ACCESS WITH TYPE"
                ]
            }
        },
        "outputGraph": {
            "template": "inbound-calls-graph"
        },
        "children": []
    },
    {
        "name": "GDG Files: Programs/Utilities manipulating Data",
        "id": 1202015,
        "category": "TASK",
        "branchType": null,
        "rationales": {
            "Agnostic": "Please review each Cobol Program reading/writing in GDG file in order to plan replacement using SQL statements.\n"
        },
        "remediations": {},
        "populations": {
            "Mainframe": "GDG dataset : (EXISTS {MATCH (o:Object)-[:Property {value:'GDG Dataset'}]->(op:ObjectProperty)WHERE op.Id IN ['141023']})\n\n\nEach GDG dataset must be called by Cobol Program with an ACCESS/reference link\n"
        },
        "description": "For Each used GDG Dataset, we display the access mode(s) (Read/Write mode) and Calling Cobol Program",
        "impacts": [
            "replatform",
            "rehost",
            "refactor"
        ],
        "effort": "moderate",
        "parents": [
            1202305
        ],
        "outputTable": {
            "template": "inbound-calls-table",
            "table": {
                "head": [
                    "GDG FILE",
                    "Cobol Program/Paragraph",
                    "ACCESS WITH TYPE"
                ],
                "body": [
                    "GDG File FullNAME",
                    "CALLER Cobol Program/Paragraph FullNAME",
                    "ACCESS WITH TYPE"
                ]
            }
        },
        "outputGraph": {
            "template": "inbound-calls-graph"
        },
        "children": []
    },
    {
        "name": "Cobol Program using Advanced Program To Program Communication ( APPC) Protocol",
        "id": 1202018,
        "category": "TASK",
        "branchType": null,
        "rationales": {
            "Agnostic": "APPC (Advanced Program-to-Program Communications protocol, also known as LU 6.2, was introduced by IBM in 1982) is used to address the exchange of data between two peer programs that are located either in the same computer or in two systems connected by the network of the z/OS operating system. \n Several APIs were developed for programming languages such as COBOL, PL/I, C or REXX.\nAPPC software is available for many different IBM and non-IBM operating systems, either as part of the operating system or as a separate software package\n\n\nCobol programs using this communication protocol should be identified before migration. this will be helpful to check if the used APIs are compatible on the target Cloud Environment \n\n\n"
        },
        "remediations": {},
        "populations": {
            "Mainframe": "APPC utilities: http://rulesmanager/techno-guru/#page=1&sort=popularity&technologies=match%20APPC\nCobol Program calling APPC utilities"
        },
        "description": "Identify all Cobol Programs using APPC communication protocol.",
        "impacts": [
            "rehost",
            "replatform",
            "review"
        ],
        "effort": "low",
        "parents": [
            1202313
        ],
        "outputTable": {
            "template": "outbound-calls-table",
            "table": {
                "head": [
                    "APPC UTILITY",
                    "CALLER FULLNAME",
                    "CALLER OBJECT TYPE"
                ],
                "body": [
                    "CALLER FullNAME",
                    "Utility FullNAME",
                    "CALLEE OBJECT TYPE"
                ]
            }
        },
        "outputGraph": {
            "template": "outbound-calls-graph"
        },
        "children": []
    },
    {
        "name": "Cobol Program using CICS Transaction Gateway (CTG)",
        "id": 1202019,
        "category": "TASK",
        "branchType": null,
        "rationales": {
            "Agnostic": "CTG (CICS Transaction Gateway) provides resource adapters to connect Java client programs to existing CICS programs in a CICS server. CICS programs using this communication protocol should be identified before migration "
        },
        "remediations": {},
        "populations": {
            "Mainframe": "Cobol Program calling CTG utilities\nhttp://rulesmanager/techno-guru/#page=1&sort=popularity&technologies=match%20CTG"
        },
        "description": "Identify all Cobol Programs using CTG communication protocol.",
        "impacts": [
            "rehost",
            "replatform",
            "review",
            "refactor"
        ],
        "effort": "low",
        "parents": [
            1202313
        ],
        "outputTable": {
            "template": "outbound-calls-table",
            "table": {
                "head": [
                    "CTG UTILITY",
                    "CALLER FULLNAME",
                    "CALLER OBJECT TYPE"
                ],
                "body": [
                    "CALLER FullNAME",
                    "Utility FullNAME",
                    "CALLEE OBJECT TYPE"
                ]
            }
        },
        "outputGraph": {
            "template": "outbound-calls-graph"
        },
        "children": []
    },
    {
        "name": "Cobol Programs using IBM MQSeries - MQ Utilities",
        "id": 1202020,
        "category": "TASK",
        "branchType": null,
        "rationales": {
            "Agnostic": "Identifying COBOL programs using IBM MQ utilities allows the migration team to assess the scope and complexity of the messaging infrastructure changes required to support these programs in the cloud. So they can plan for all necessary changes and code refactoring to integrate the new cloud service replicating IBM MQSeries."
        },
        "remediations": {},
        "populations": {
            "Mainframe": "Cobol Programs (InternalType=CAST_COBOL_SavedProgram) calling MQ utilities\nJCL Job or JCL Procedure Calling MQ utilities"
        },
        "description": "Identify all Cobol Programs, JCL Jobs and JCL Procedures Calling an IBM MQ Utility",
        "impacts": [
            "rehost",
            "replatform",
            "review",
            "refactor"
        ],
        "effort": "moderate",
        "parents": [
            1202312
        ],
        "outputTable": {
            "template": "outbound-calls-table",
            "table": {
                "head": [
                    "MQ UTILITY",
                    "CALLER FULLNAME",
                    "CALLER OBJECT TYPE"
                ],
                "body": [
                    "CALLEE FullNAME",
                    "CALLER FULLNAME",
                    "CALLEE OBJECT TYPE"
                ]
            }
        },
        "outputGraph": {
            "template": "outbound-calls-graph"
        },
        "children": []
    },
    {
        "name": "Cobol Programs candidate for Cloud function conversion: Data-independent programs",
        "id": 1202027,
        "category": "TASK",
        "branchType": null,
        "rationales": {
            "Agnostic": "Data-independent programs: Cobol Programs accessing NOT SHRAED DB table/Data file are good candidate for aws Lambda conversion"
        },
        "remediations": {},
        "populations": {
            "Mainframe": "CAST_COBOL_SavedProgram with the property 'Number of Transaction' = 1\nThe Entry Point of the transaction should have the property 'Number of Transaction' = 1\n\nThe program is accessing a NOT SHARED Database table or NOT Shared DataSet File Storage\nNOT SHARED Database table or NOT SHARED DataSet File Storage: means that the table or NOT the DataSet File Storage is accessed only by this program \n\n\n"
        },
        "description": "Among the candidates for AWS Lambda function conversion, This rule identifies the program accessing NOT Shared Database table or Any DataSet file Storage",
        "impacts": [
            "refactor"
        ],
        "effort": "moderate",
        "parents": [
            1202300
        ],
        "outputTable": {
            "template": "none",
            "table": {
                "head": [
                    "COBOL PROGRAM FULLNAME",
                    "TRANSACTION NAME"
                ],
                "body": null
            }
        },
        "outputGraph": {
            "template": "none"
        },
        "children": []
    },
    {
        "name": "ESDS VSAM Files: Programs/Utilities manipulating Data",
        "id": 1202029,
        "category": "TASK",
        "branchType": null,
        "rationales": {
            "Azure": "ESDS VSAM data sets files are used to store data in sequential order, similar to KSDS VSAM data sets files, but they do not have a key field to uniquely identify each record.\n\nIn ESDS VSAM files, records are stored in the order in which they are written to the file, and each record is assigned a relative byte address (RBA) that indicates its position in the file. \nRecords can be accessed sequentially by reading the file in order, but random access to individual records is not efficient.\n\nESDS VSAM files are particularly useful for applications that require high-speed data transfer and bulk data processing, such as batch processing jobs. They are also efficient for storing large amounts of data that are frequently updated or appended, as new records are simply added to the end of the file.\n\n\nWhen moving to Azure Cloud, VSAM files are, usually, replicated to Azure SQL Database \n\nVSAM has also many characteristics of a NO-SQL database and than can be moved to Cosmos DB \nhttps://azure.microsoft.com/mediahandler/files/resourcefiles/vsam-to-azure-cosmos-db/VSAM%20to%20Azure%20Cosmos%20DB.pdf) ",
            "Amazon Web Services": "ESDS VSAM data sets files are used to store data in sequential order, similar to KSDS VSAM data sets files, but they do not have a key field to uniquely identify each record.\n\nIn ESDS VSAM files, records are stored in the order in which they are written to the file, and each record is assigned a relative byte address (RBA) that indicates its position in the file. \nRecords can be accessed sequentially by reading the file in order, but random access to individual records is not efficient.\n\nESDS VSAM files are particularly useful for applications that require high-speed data transfer and bulk data processing, such as batch processing jobs. They are also efficient for storing large amounts of data that are frequently updated or appended, as new records are simply added to the end of the file.\n\n\nWhen moving to AWS, VSAM files are, usually, replicated to Amazon RDS\nhttps://docs.aws.amazon.com/prescriptive-guidance/latest/patterns/migrate-and-replicate-vsam-files-to-amazon-rds-or-amazon-msk-using-connect-from-precisely.html\n\n\n",
            "Gougle Cloud Platform": "ESDS VSAM data sets files are used to store data in sequential order, similar to KSDS VSAM data sets files, but they do not have a key field to uniquely identify each record.\n\nIn ESDS VSAM files, records are stored in the order in which they are written to the file, and each record is assigned a relative byte address (RBA) that indicates its position in the file. \nRecords can be accessed sequentially by reading the file in order, but random access to individual records is not efficient.\n\nESDS VSAM files are particularly useful for applications that require high-speed data transfer and bulk data processing, such as batch processing jobs. They are also efficient for storing large amounts of data that are frequently updated or appended, as new records are simply added to the end of the file."
        },
        "remediations": {},
        "populations": {
            "Mainframe": "List ESDS VSAM Files (EXISTS {MATCH (o:Object)-[:Property {value:'ESDS VSAM File'}]->(op:ObjectProperty)WHERE op.Id IN ['141023']})\n\nFor each ESDS VSAM dataset called by Cobol Program: ESDS VSAM dataset + Calling Program + ACCESS MODE\nFor each ESDS VSAM dataset called by Any Utility: ESDS VSAM dataset + Calling Any Utility + ACCESS MODE\n"
        },
        "description": "Listing different access modes is important to migrate ESDS VSAM files to the cloud because it helps to identify the specific operations that the application performs on the data stored in the VSAM files. \n\nThis information is critical in determining the appropriate cloud storage solution and configuring it to provide the necessary performance and reliability.\n\nThis rule lists of all Cobol Programs and utilities accessing ESDS VSAM files and access modes",
        "impacts": [
            "replatform"
        ],
        "effort": "high",
        "parents": [
            1202311
        ],
        "outputTable": {
            "template": "inbound-calls-table",
            "table": {
                "head": [
                    "ESDS VSAM FILE",
                    "Cobol Program/Paragraph",
                    "ACCESS WITH TYPE"
                ],
                "body": [
                    "ESDS VSAM File FullNAME",
                    "Caller Cobol Program/Paragraph FullNAME",
                    "ACCESS WITH TYPE"
                ]
            }
        },
        "outputGraph": {
            "template": "inbound-calls-graph"
        },
        "children": []
    },
    {
        "name": "GDG Files Defined but not storing data",
        "id": 1202030,
        "category": "TASK",
        "branchType": null,
        "rationales": {
            "Agnostic": "Identifying defined Dataset files that are not used for data Storage is useful to optimize migration effort and cost. \nThese datasets may be the symptom of Dead Code and so good candidates to be retired"
        },
        "remediations": {},
        "populations": {
            "Mainframe": "GDG Dataset : (EXISTS {MATCH (o:Object)-[:Property {value:'GDG Dataset'}]->(op:ObjectProperty)WHERE op.Id IN ['141023']})\n\n\nGDG dataset Called by Cobol Program with DEFINE link ONLY\nOR GDG dataset called ONLY by JCL Steps"
        },
        "description": "This rule checks GDG Files defined but not used for data storage.",
        "impacts": [
            "retire",
            "refactor",
            "replatform",
            "rehost"
        ],
        "effort": "moderate",
        "parents": [
            1202305
        ],
        "outputTable": {
            "template": "inbound-calls-table",
            "table": {
                "head": [
                    "GDG FILE",
                    "CALLER",
                    "ACCESS WITH TYPE"
                ],
                "body": [
                    "GDG FULLNAME",
                    "CALLER FULLNAME",
                    "ACCESS WITH TYPE"
                ]
            }
        },
        "outputGraph": {
            "template": "violations-graph"
        },
        "children": []
    },
    {
        "name": "JCL Job/ JCL Procedure using CA-7 Scheduler (ISV  Broadcom)",
        "id": 1202031,
        "category": "TASK",
        "branchType": null,
        "rationales": {
            "Azure": "CA-7 is a job scheduling/workflow automation tool packaged by ISV Broadcom. \n\nCA-7 job scheduler can be migrated to Azure Scheduler or Azure Logic Apps that provides triggers for starting and running workflows based on the interval and frequency of recurrence that user specifies.\n\n\nIdentifying the used scheduler and all JCL jobs, JCL procedures, and eventually Cobol Programs handled and monitored by this scheduler allows the migration team to assess the scope and complexity of the scheduling/workflow automation \ninfrastructure changes required to migrate to the cloud environment. So they can plan for all necessary changes and code refactoring to integrate the new cloud service replicating the CA-7 scheduler.",
            "Amazon Web Services": "CA-7 is a job scheduling/workflow automation tool packaged by ISV Broadcom. \n\nCA-7 job scheduler can be migrated to Apache Airflow in AWS ( + additional service as Amazon S3 for the storage of Airflow DAGs and data files…) \n\n\nIdentifying all JCL jobs, JCL procedures, and eventually Cobol Programs handled and monitored by CA-7 scheduler allows the migration team to assess the scope and complexity of the scheduling/workflow automation \ninfrastructure changes required to migrate to the cloud environment."
        },
        "remediations": {},
        "populations": {
            "Mainframe": "CA-7 utilities : http://rulesmanager/techno-guru/#page=1&sort=popularity&technologies=like%20%25CA%257%25%20\nJCL Job/Procedure calling CA-7 utilities + Cobol program called by this JCL Job/Procedure"
        },
        "description": "Identify JCL Jobs and JCL Procedures calling CA-7 Scheduler Utilities\nIdentify Programs launched by these jobs",
        "impacts": [
            "replatform",
            "review"
        ],
        "effort": "moderate",
        "parents": [
            1202307
        ],
        "outputTable": {
            "template": "outbound-calls-table",
            "table": {
                "head": [
                    "CA-7 UTILITY",
                    "CALLER FULLNAME",
                    "CALLER OBJECT TYPE"
                ],
                "body": [
                    "Utility FullNAME",
                    "CALLER FullNAME",
                    "CALLER OBJECT TYPE"
                ]
            }
        },
        "outputGraph": {
            "template": "outbound-calls-graph"
        },
        "children": []
    },
    {
        "name": "JCL Job/ JCL Procedure using Control M Scheduler (ISV BMC)",
        "id": 1202032,
        "category": "TASK",
        "branchType": null,
        "rationales": {
            "Azure": "BMC provides specific workflow orchestration for Control-M used for mainframe applications on Azure VM or with other Azure services \n\nMore details can be found here: \nhttps://www.bmc.com/it-solutions/control-m-integrations/azure-vm-for-control-m.html\nhttps://docs.bmc.com/docs/ctm_integrations/control-m-for-azure-functions-1066864662.html\n...\n\n\nIdentifying all JCL jobs, JCL procedures, and eventually Cobol Programs handled and monitored by Control-M scheduler allows the migration team to assess the scope and complexity of the scheduling/workflow automation \ninfrastructure changes required to migrate to the cloud environment.",
            "Amazon Web Services": "Control-M is a BMC tool for workflow orchestration on-premises or as a service. It's used to build, define, schedule, manage, and monitor production workflows, ensuring visibility, reliability, and improving SLAs.\n\nBMC provides specific workflow orchestration for Control-M used for mainframe applications on AWS cloud environment. \nThis workflow integrates some AWS native services: For data processing, Amazon EMR is used because it gives access to a wide variety of tools in the Apache Hadoop ecosystem for big data processing and analytics….  \nMore details can be found here: https://aws.amazon.com/blogs/apn/how-to-orchestrate-a-data-pipeline-on-aws-with-control-m-from-bmc-software/\n\n\nIdentifying all JCL jobs, JCL procedures, and eventually Cobol Programs handled and monitored by Control-M scheduler allows the migration team to assess the scope and complexity of the scheduling/workflow automation \ninfrastructure changes required to migrate to the cloud environment.",
            "Gougle Cloud Platform": "Control-M is a BMC tool for workflow orchestration on-premises or as a service. It's used to build, define, schedule, manage, and monitor production workflows, ensuring visibility, reliability, and improving SLAs.\n\n\nThe Google VM provides a plugin for Control-M to enable the integration of Google VM jobs with existing Control-M workflows. \n\n\nMore details can be found here: \nhttps://www.bmc.com/it-solutions/control-m-integrations/gcp-vm.html\nhttps://docs.bmc.com/docs/ctm_integrations/control-m-for-google-virtual-machine-1125418845.html\nhttps://docs.bmc.com/docs/ctm_integrations/control-m-for-google-batch-1197120468.html\n\n\nIdentifying all JCL jobs, JCL procedures, and eventually Cobol Programs handled and monitored by Control-M scheduler allows the migration team to assess the scope and complexity of the scheduling/workflow automation \ninfrastructure changes required to migrate to the cloud environment.\n"
        },
        "remediations": {},
        "populations": {
            "Mainframe": "Control M Utilities : http://rulesmanager/techno-guru/#page=1&sort=popularity&technologies=like%20%25Control%25M%25\nJCL Job/Procedure calling 'Control M' utilities + Cobol program called by this JCL Job/Procedure"
        },
        "description": "Identify all JCL Jobs and JCL Procedures calling Control-M Scheduler utilities\nIdentify Programs launched by these jobs",
        "impacts": [
            "rehost",
            "replatform",
            "review"
        ],
        "effort": "moderate",
        "parents": [
            1202307
        ],
        "outputTable": {
            "template": "outbound-calls-table",
            "table": {
                "head": [
                    "Control M  UTILITY",
                    "CALLER FULLNAME",
                    "CALLER OBJECT TYPE"
                ],
                "body": [
                    "Utility FullNAME",
                    "CALLER FullNAME",
                    "CALLER OBJECT TYPE"
                ]
            }
        },
        "outputGraph": {
            "template": "outbound-calls-graph"
        },
        "children": []
    },
    {
        "name": "JCL Job/ JCL Procedure using EPS Scheduler",
        "id": 1202033,
        "category": "TASK",
        "branchType": null,
        "rationales": {
            "Agnostic": "EPS (Execution Processing System) Scheduler is a job scheduling software for mainframe computers. It is a product of CA Technologies. The EPS Scheduler is designed to manage batch processing jobs on a mainframe system. It provides a centralized interface for scheduling, monitoring, and managing batch jobs.\n\nWhen moving to cloud envrironmrnt, it's recommanded to replicate this scheduler by similar environment-specific schedulers such as AWS\nBatch and Azure Scheduler.\n\n\nIdentifying all JCL jobs, JCL procedures, and eventually Cobol Programs handled and monitored by EPS scheduler allows the migration team to assess the scope and complexity of the scheduling/workflow automation \ninfrastructure changes required to migrate to the cloud environment.\n\n\n\n"
        },
        "remediations": {},
        "populations": {
            "Mainframe": "Cobol Program calling ESP Sceduler utilities\nESP Scheduler : http://rulesmanager/techno-guru/#page=1&sort=popularity&technology2=like%20%25ESP%25\nJCL Job/Procedure calling 'EPS' utilities + Cobol program called by this JCL Job/Procedure"
        },
        "description": "Identify JCL Jobs and JCL Procedures calling EPS Scheduler Utilities\nIdentify Programs launched by these jobs",
        "impacts": [
            "refactor",
            "rehost",
            "replatform",
            "review"
        ],
        "effort": "moderate",
        "parents": [
            1202307
        ],
        "outputTable": {
            "template": "outbound-calls-table",
            "table": {
                "head": [
                    "EPS UTILITY",
                    "CALLER FULLNAME",
                    "CALLER OBJECT TYPE"
                ],
                "body": [
                    "Utility FullNAME",
                    "CALLER  FullNAME",
                    "CALLER OBJECT TYPE"
                ]
            }
        },
        "outputGraph": {
            "template": "outbound-calls-graph"
        },
        "children": []
    },
    {
        "name": "JCL Job/ JCL Procedure using TWS Scheduler",
        "id": 1202034,
        "category": "TASK",
        "branchType": null,
        "rationales": {
            "Azure": "IBM Tivoli Workload Scheduler is the production workload manager for distributed platforms. \n\n\nIdentifying all JCL jobs, JCL procedures, and eventually Cobol Programs handled and monitored by TWS scheduler allows the migration team to assess the scope and complexity of the scheduling/workflow automation \ninfrastructure changes required to migrate to the cloud environment.",
            "Amazon Web Services": "IBM Tivoli Workload Scheduler is the production workload manager for distributed platforms. Some of IBM's work to bring its product line into the AWS environment is underway. \n\nMore details can be found here https://aws.amazon.com/blogs/aws/ibm-tivoli-now-available-on-amazon-ec2/\n\nAll JCL and programs using this scheduler should be identified to consider the new adoption or replacement with an equivalent cloud service. ",
            "Gougle Cloud Platform": "IBM Tivoli Workload Scheduler is the production workload manager for distributed platforms. \n\nIdentifying all JCL jobs, JCL procedures, and eventually Cobol Programs handled and monitored by TWS scheduler allows the migration team to assess the scope and complexity of the scheduling/workflow automation \ninfrastructure changes required to migrate to the cloud environment."
        },
        "remediations": {},
        "populations": {
            "Mainframe": "JCL Job/Procedure calling 'TWS' utilities + Cobol program called by this JCL Job/Procedure\nTWS scheduler utilities : http://rulesmanager/techno-guru/#page=1&sort=popularity&technologies=match%20tws"
        },
        "description": "Identify JCL Jobs and JCL Procedures calling TWS Scheduler Utilities\nIdentify Programs launched by these jobs",
        "impacts": [
            "rehost",
            "replatform",
            "review"
        ],
        "effort": "moderate",
        "parents": [
            1202307
        ],
        "outputTable": {
            "template": "outbound-calls-table",
            "table": {
                "head": [
                    "TWS UTILITY",
                    "CALLER FULLNAME",
                    "CALLER OBJECT TYPE"
                ],
                "body": [
                    "UTILITY FullNAME",
                    "CALLER FullNAME",
                    "CALLER OBJECT TYPE:"
                ]
            }
        },
        "outputGraph": {
            "template": "outbound-calls-graph"
        },
        "children": []
    },
    {
        "name": "JCL Job/Cobol Program  using 3rd Party Products/tools : CA-ACF2 (ISV Broadcom)",
        "id": 1202035,
        "category": "TASK",
        "branchType": null,
        "rationales": {
            "Azure": "CA-ACF2 (ISV Broadcom) is a Security and Access Control Tool for mainframe applications on the z/OS environment. During application migration to Azure, this tool will be likely mapped to an equivalent native cloud Service. \n\nAzure Active Directory (Azure AD) is Microsoft’s vehicle for providing identity management in the cloud: it can be an Alternative service\nWhile there are other possibilities, like LDAP, you’ll most likely map your mainframe-based identity management system (IDs, passwords, permissions, etc.) to Azure AD.\n\nAll JCL jobs/Procedures and eventually Cobol program using this tool should be identified since specific integration with Cloud service will be required.\n\n\n\n\n\n",
            "Amazon Web Services": "CA-ACF2 (ISV Broadcom) is a Security and Access Control Tool for mainframe applications on the z/OS environment. During application migration to the AWS, this tool will be likely mapped to an equivalent native cloud Service. AWS Identity and Access Management (IAM) can be an Alternative service and AWS. All JCL jobs/Procedures and eventually Cobol program using this tool should be identified since specific integration with Cloud service will be required."
        },
        "remediations": {},
        "populations": {
            "Mainframe": "CA-ACF2 utilities: http://rulesmanager/techno-guru/#page=1&sort=popularity&technologies=like%20%25CA%25ACF2%25%20\nJCL Job/Procedure calling CA-ACF2 utilities + Cobol program called by this JCL Job/Procedure"
        },
        "description": "",
        "impacts": [
            "rehost",
            "replatform",
            "review"
        ],
        "effort": "moderate",
        "parents": [
            1202306
        ],
        "outputTable": {
            "template": "outbound-calls-table",
            "table": {
                "head": [
                    "CA-ACF2 UTILITY",
                    "CALLER FULLNAME",
                    "CALLER OBJECT TYPE"
                ],
                "body": [
                    "Utility  FullNAME",
                    "CALLER FullNAME",
                    "CALLER OBJECT TYPE"
                ]
            }
        },
        "outputGraph": {
            "template": "outbound-calls-graph"
        },
        "children": []
    },
    {
        "name": "JCL Job/JCL Procedure doing Data Transfer with EzSocket Protocol",
        "id": 1202036,
        "category": "TASK",
        "branchType": null,
        "rationales": {
            "Agnostic": "JCL jobs and Procedures doing data and/or file transfer should be identified and also dependent programs. When moving to cloud platform, this protocol is replaced with cloud-native data transfer tool. Minimal Code changes or refactoring are necessary to integrate the equivalent cloud native service."
        },
        "remediations": {},
        "populations": {
            "Mainframe": "EzSocket utilities : http://rulesmanager/techno-guru/#page=1&sort=popularity&technologies=match%20EzSocket%20 \nJCL Job/JCL Procedure calling EzSocket utilities + Cobol Programs calling these JCL Job/JCL Procedure"
        },
        "description": "Identify JCL Jobs and JCL Procedure calling EzSocket utilities",
        "impacts": [
            "rehost",
            "review"
        ],
        "effort": "moderate",
        "parents": [
            1202309
        ],
        "outputTable": {
            "template": "outbound-calls-table",
            "table": {
                "head": [
                    "EzSocket UTILITY",
                    "CALLER FULLNAME",
                    "CALLER OBJECT TYPE"
                ],
                "body": [
                    "Utility FullNAME ",
                    "CALLER FullNAME",
                    "CALLEr OBJECT TYPE:"
                ]
            }
        },
        "outputGraph": {
            "template": "outbound-calls-graph"
        },
        "children": []
    },
    {
        "name": "JCL Job/Cobol Program using 3rd Party Products/tools : AR/CTL (ISV BMC)",
        "id": 1202037,
        "category": "TASK",
        "branchType": null,
        "rationales": {
            "Azure": "AR/CTL products provide facilities for security implementation of application programs that run in the z/OS system environment. \nWhen migration mainframe application to the cloud, this tool will be likely mapped to an equivalent cloud Service: AWS Identity and Access Management (IAM) can be an Alternative service. \n\nAll JCL/programs using this tool need to be identified to consider adoption and deployment with the replication aws service.\n",
            "Amazon Web Services": "AR/CTL products provide facilities for security implementation of application programs that run in the z/OS system environment. \nWhen migration mainframe application to the cloud, this tool will be likely mapped to an equivalent cloud Service: AWS Identity and Access Management (IAM) can be an Alternative service. \n\nAll JCL/programs using this tool need to be identified to consider adoption and deployment with the replication aws service."
        },
        "remediations": {},
        "populations": {
            "Mainframe": "AR/CTL utilitis \nhttp://rulesmanager/techno-guru/#page=1&sort=popularity&technologies=match%20ctl\nJCL Job/Procedure calling AR/CTL utilities + Cobol program called by this JCL Job/Procedure"
        },
        "description": "",
        "impacts": [
            "rehost",
            "replatform",
            "review"
        ],
        "effort": "moderate",
        "parents": [
            1202306
        ],
        "outputTable": {
            "template": "outbound-calls-table",
            "table": {
                "head": [
                    "AR/CTL UTILITY",
                    "CALLER FULLNAME",
                    "CALLER OBJECT TYPE"
                ],
                "body": [
                    "Utility FullNAME",
                    "CALLER FullNAME",
                    "CALLER OBJECT TYPE"
                ]
            }
        },
        "outputGraph": {
            "template": "outbound-calls-graph"
        },
        "children": []
    },
    {
        "name": "JCL Job/Cobol Program using 3rd Party Products/tools : StreamWeaver (ISV BMC)",
        "id": 1202038,
        "category": "TASK",
        "branchType": null,
        "rationales": {
            "Azure": "StreamWeaver (ISV BMC) printing tool can be replaced by a cloud-native solution\nAll JCL/programs using this tool need to be identified to consider adoption and deployment with new cloud service/solution",
            "Amazon Web Services": "StreamWeaver (ISV BMC) printing tool can be replaced by a cloud-native solution as LRS printing solution on AWS.\nAll JCL/programs using this tool need to be identified to consider adoption and deployment with new cloud service/solution",
            "Gougle Cloud Platform": "StreamWeaver (ISV BMC) printing tool can be replaced by a cloud-native solution\nAll JCL/programs using this tool need to be identified to consider adoption and deployment with new cloud service/solution"
        },
        "remediations": {},
        "populations": {
            "Mainframe": "JCL Job/Procedure calling 'StreamWeaver' utilities + Cobol program called by this JCL Job/Procedure\nStream Weaver Utilities : http://rulesmanager/techno-guru/#page=1&sort=popularity&technologies=match%20StreamWeaver"
        },
        "description": "Identify JCL Jobs and JCL Procedures calling StreamWeaver Utilities\nIdentify Programs launched by these jobs",
        "impacts": [
            "replatform",
            "review",
            "refactor"
        ],
        "effort": "moderate",
        "parents": [
            1202308
        ],
        "outputTable": {
            "template": "outbound-calls-table",
            "table": {
                "head": [
                    "StreamWeaver UTILITY",
                    "CALLER FULLNAME",
                    "CALLER OBJECT TYPE"
                ],
                "body": [
                    "UTILITY FullNAME",
                    "CALLER FullNAME",
                    "CALLER OBJECT TYPE"
                ]
            }
        },
        "outputGraph": {
            "template": "outbound-calls-graph"
        },
        "children": []
    },
    {
        "name": "JCL Job/JCL Procedure doing Data Transfer with FTP Protocol",
        "id": 1202039,
        "category": "TASK",
        "branchType": null,
        "rationales": {
            "Azure": "The FTP protocol in z/OS has many similarities to FTP on other platforms. There are some aspects of FTP on z/OS® that are unique to the z/OS® platform.\n\nUsing FTP protocol in the Azure environment will require integration of additional Azure native services as Azure AD and Azure storage services that can be used to store and access data over the selected protocol (Azure Blob Service to store and access files as objects over the selected protocol or Azure files service to store and access your files system over the selected protocol). \n\n\nWhen moving mainframe applications using FTP protocol for data transfer, many changes and modifications will be required to adapt this protocol to the cloud environment specifications \n\nIdentifying the protocol beforehand and all JCL jobs and programs involved can help ensure to asses complexity of the Integration of data transfer services/protocols on the target cloud environment.",
            "Amazon Web Services": "The FTP protocol in z/OS has many similarities to FTP on other platforms. There are some aspects of FTP on z/OS® that are unique to the z/OS® platform.\n\nUsing FTP protocol in AWS environment will require integration of additional AWS native services as identity providers and AWS storage services that can be used to store and access data over the selected protocol (Amazon S3 to store and access files as objects over the selected protocol or Amazon EFS to store and access your files system over the selected protocol). \n\nIntegration of these services will require code change for all JCL Jobs and Cobol Programs using FTP Protocol",
            "Gougle Cloud Platform": "The FTP protocol in z/OS has many similarities to FTP on other platforms. There are some aspects of FTP on z/OS® that are unique to the z/OS® platform.\n\n\nWhen moving mainframe applications using FTP protocol for data transfer, many changes and modifications will be required to adapt this protocol to the cloud environment specifications \n\nIdentifying the protocol beforehand and all JCL jobs and programs involved can help ensure to asses complexity of the Integration of data transfer services/protocols on the target cloud environment."
        },
        "remediations": {},
        "populations": {
            "Mainframe": "FTP utilities : http://rulesmanager/techno-guru/#page=1&sort=popularity&technologies=match%20FTP\nJCL Job/JCL Procedure calling FTP utilities"
        },
        "description": "Identify JCL Jobs and JCL Procedure calling FTP utilities",
        "impacts": [
            "rehost",
            "review",
            "replatform"
        ],
        "effort": "moderate",
        "parents": [
            1202309
        ],
        "outputTable": {
            "template": "outbound-calls-table",
            "table": {
                "head": [
                    "FTP UTILITY",
                    "CALLER FULLNAME",
                    "CALLER OBJECT TYPE"
                ],
                "body": [
                    "Utility FullNAME ",
                    "CALLER FullNAME",
                    "CALLER OBJECT TYPE"
                ]
            }
        },
        "outputGraph": {
            "template": "outbound-calls-graph"
        },
        "children": []
    },
    {
        "name": "JCL Job/JCL Procedure doing Data Transfer with NDM Protocol",
        "id": 1202040,
        "category": "TASK",
        "branchType": null,
        "rationales": {
            "Azure": "Connect:Direct—originally named Network Data Mover (NDM) is a computer software product that transfers files between mainframe computers and/or midrange computers.\n\n\nAll JCL Jobs and/or Procedures using NDM Protocol need to be identified and reviewed to check the compatibility of used utilities and to ensure integration with Equivalent AWS native service for data transfer.",
            "Amazon Web Services": "Connect:Direct—originally named Network Data Mover (NDM) is a computer software product that transfers files between mainframe computers and/or midrange computers.\n\n\nIBM Sterling Connect:Direct 5.3.0 introduces support for reading and writing data using the Amazon S3 Object Store. (https://www.ibm.com/docs/en/connect-direct/5.3.0?topic=usospcdu-setting-up-connectdirect-node-s3-object-store-providers)\n\n\nAll JCL Jobs and/or Procedures using NDM Protocol need to be identified and reviewed to check the compatibility of used utilities and to ensure integration with Equivalent AWS native service for data transfer.",
            "Gougle Cloud Platform": "Connect:Direct—originally named Network Data Mover (NDM) is a computer software product that transfers files between mainframe computers and/or midrange computers.\n\n\nAll JCL Jobs and/or Procedures using NDM Protocol need to be identified and reviewed to check the compatibility of used utilities and to ensure integration with Equivalent AWS native service for data transfer."
        },
        "remediations": {},
        "populations": {
            "Mainframe": "JCL Job/JCL Procedure calling  NDMutilities\nNDM utilities : http://rulesmanager/techno-guru/#page=1&sort=popularity&technologies=match%20NDM"
        },
        "description": "Identify JCL Jobs and JCL Procedure calling NDM utilities",
        "impacts": [
            "rehost",
            "review",
            "replatform",
            "refactor"
        ],
        "effort": "moderate",
        "parents": [
            1202309
        ],
        "outputTable": {
            "template": "outbound-calls-table",
            "table": {
                "head": [
                    "NDM UTILITY",
                    "CALLER FULLNAME",
                    "CALLER OBJECT TYPE"
                ],
                "body": [
                    "Utility FullNAME ",
                    "CALLER FullNAME",
                    "CALLER OBJECT TYPE"
                ]
            }
        },
        "outputGraph": {
            "template": "outbound-calls-graph"
        },
        "children": []
    },
    {
        "name": "JCL Job/JCL Procedure doing data Data Transfer with Co:Z SFTP Protocol",
        "id": 1202041,
        "category": "TASK",
        "branchType": null,
        "rationales": {
            "Azure": "Co:Z SFTP is a file transfer protocol used to transfer files between two systems over a network. It is a proprietary protocol developed by IBM for use with their Co:Z toolkit.\n\nWhen moving mainframe applications using Co:Z SFTP protocol, many changes and modifications will be required to replace it by equivalent native cloud transfer services/protocols.\n\nIdentifying the protocol beforehand and all JCL jobs and programs involved can help to assess the complexity of the Integration of data transfer services/protocols on the target cloud environment.\n",
            "Amazon Web Services": "Co:Z SFTP is a file transfer protocol used to transfer files between two systems over a network. It is a proprietary protocol developed by IBM for use with their Co:Z toolkit.\n\nWhen moving mainframe applications using Co:Z SFTP protocol, many changes and modifications will be required to replace it by an equivalent native cloud transfer services/protoles \n\n\nAWS Transfer Family service enables users to securely scale file transfers to Amazon S3 and Amazon EFS using SFTP, FTPS, and FTP protocols. Integration of this service will require minimum code change for all JCL Jobs/Procedures using NDM Protocol.\n\nIdentifying the protocol beforehand and all JCL jobs and programs involved can help to assess the complexity of the Integration of data transfer services/protocols on the target cloud environment.\n",
            "Gougle Cloud Platform": "Co:Z SFTP is a file transfer protocol used to transfer files between two systems over a network. It is a proprietary protocol developed by IBM for use with their Co:Z toolkit.\n\nWhen moving mainframe applications using Co:Z SFTP protocol, many changes and modifications will be required to replace it by equivalent native cloud transfer services/protocols.\n\nIdentifying the protocol beforehand and all JCL jobs and programs involved can help to assess the complexity of the Integration of data transfer services/protocols on the target cloud environment.\n\n"
        },
        "remediations": {},
        "populations": {
            "Mainframe": "Co:Z SFTP utilities: http://rulesmanager/techno-guru/#page=1&sort=popularity&technologies=match%20SFTP\nJCL Job/JCL Procedure calling Co:Z SFTP utilities"
        },
        "description": "Identify JCL Jobs and JCL Procedure calling Co:Z SFTP utilities",
        "impacts": [
            "rehost",
            "review",
            "replatform",
            "refactor"
        ],
        "effort": "moderate",
        "parents": [
            1202309
        ],
        "outputTable": {
            "template": "outbound-calls-table",
            "table": {
                "head": [
                    "Co:Z SFTP UTILITY",
                    "CALLER FULLNAME",
                    "CALLER OBJECT TYPE"
                ],
                "body": [
                    "Utility FullNAME",
                    "CALLER FullNAME",
                    "CALLER OBJECT TYPE"
                ]
            }
        },
        "outputGraph": {
            "template": "outbound-calls-graph"
        },
        "children": []
    },
    {
        "name": "KSDS VSAM Files: Programs/Utilities manipulating Data",
        "id": 1202042,
        "category": "TASK",
        "branchType": null,
        "rationales": {
            "Azure": "KSDS stands for Key Sequenced Data Set, which is a type of file organization used in the IBM Virtual Storage Access Method (VSAM) data storage system. KSDS VSAM files are used to store and manage data in a sequential order based on a key field, which is used to uniquely identify each record in the file.\n\nIn KSDS VSAM files, records are stored based on their key values in ascending order, and each record has a unique key value. The records can be accessed directly using their key values, which makes KSDS VSAM files particularly useful for applications that require rapid access to specific records.\n\nKSDS VSAM files also support dynamic record sizing, which means that records can have different lengths, and records can be added or deleted from the file without having to reorganize the entire file.\n\nOverall, KSDS VSAM files have been widely used in IBM mainframe environments for many years as a reliable and efficient way to store and manage large volumes of data.\n\nWhen moving to Azure Cloud, VSAM files are, usually, replicated to Azure SQL Database \n\nVSAM has also many characteristics of a NO-SQL database and than can be moved to Cosmos DB \nhttps://azure.microsoft.com/mediahandler/files/resourcefiles/vsam-to-azure-cosmos-db/VSAM%20to%20Azure%20Cosmos%20DB.pdf) ",
            "Amazon Web Services": "KSDS stands for Key Sequenced Data Set, which is a type of file organization used in the IBM Virtual Storage Access Method (VSAM) data storage system. KSDS VSAM files are used to store and manage data in a sequential order based on a key field, which is used to uniquely identify each record in the file.\n\nIn KSDS VSAM files, records are stored based on their key values in ascending order, and each record has a unique key value. The records can be accessed directly using their key values, which makes KSDS VSAM files particularly useful for applications that require rapid access to specific records.\n\nKSDS VSAM files also support dynamic record sizing, which means that records can have different lengths, and records can be added or deleted from the file without having to reorganize the entire file.\n\nOverall, KSDS VSAM files have been widely used in IBM mainframe environments for many years as a reliable and efficient way to store and manage large volumes of data.\n\n\nWhen moving to AWS, VSAM files are, usually, replicated to Amazon RDS\nhttps://docs.aws.amazon.com/prescriptive-guidance/latest/patterns/migrate-and-replicate-vsam-files-to-amazon-rds-or-amazon-msk-using-connect-from-precisely.html",
            "Gougle Cloud Platform": "KSDS stands for Key Sequenced Data Set, which is a type of file organization used in the IBM Virtual Storage Access Method (VSAM) data storage system. KSDS VSAM files are used to store and manage data in a sequential order based on a key field, which is used to uniquely identify each record in the file.\n\nIn KSDS VSAM files, records are stored based on their key values in ascending order, and each record has a unique key value. The records can be accessed directly using their key values, which makes KSDS VSAM files particularly useful for applications that require rapid access to specific records.\n\nKSDS VSAM files also support dynamic record sizing, which means that records can have different lengths, and records can be added or deleted from the file without having to reorganize the entire file.\n\nOverall, KSDS VSAM files have been widely used in IBM mainframe environments for many years as a reliable and efficient way to store and manage large volumes of data."
        },
        "remediations": {},
        "populations": {
            "Mainframe": "(EXISTS {MATCH (o:Object)-[:Property {value:'KSDS VSAM File'}]->(op:ObjectProperty)WHERE op.Id IN ['141023']})\n\nFor each KSDS VSAM dataset called by Cobol Program: KSDS VSAM dataset + Calling Program + ACCESS MODE\nFor each KSDS VSAM dataset called by Any Utility: KSDS VSAM dataset + Calling Any Utility + ACCESS MODE"
        },
        "description": "Listing CRUD (Create, Read, Update, Delete) operations is important to migrate KSDS VSAM files to the cloud because it helps to identify the specific operations that the application performs on the data stored in the VSAM files. \nThis information is critical in determining the appropriate cloud storage solution and configuring it to provide the necessary performance and reliability.\n\nList of all COBOL Programs calling KSDS VSAM file to identify access modes\nList of all utilities calling KSDS VSAM file to identify access modes",
        "impacts": [
            "replatform"
        ],
        "effort": "high",
        "parents": [
            1202311
        ],
        "outputTable": {
            "template": "inbound-calls-table",
            "table": {
                "head": [
                    "KSDS VSAM FILE",
                    "Cobol Program/Paragraph",
                    "ACCESS WITH TYPE"
                ],
                "body": [
                    "KSDS VSAM File FullNAME",
                    "Caller Cobol Program/Paragraph FullNAME",
                    "ACCESS WITH TYPE"
                ]
            }
        },
        "outputGraph": {
            "template": "inbound-calls-graph"
        },
        "children": []
    },
    {
        "name": "LDS VSAM Files : Programs/Utilities manipulating Data",
        "id": 1202043,
        "category": "TASK",
        "branchType": null,
        "rationales": {
            "Azure": "LDS stands for Linear Data Set, which is another type of file organization used in the IBM Virtual Storage Access Method (VSAM) data storage system. Unlike KSDS VSAM files that store data in a sequential order based on a key field, LDS VSAM files store data in a linear, non-sequential order.\n\nIn LDS VSAM files, each record is assigned a Relative Byte Address (RBA) that indicates its position in the file. Records can be accessed sequentially by reading the file in order, or randomly by specifying the RBA of the desired record.\n\nLDS VSAM files are particularly useful for storing large amounts of data that are frequently updated or appended, as they provide fast access to individual records without having to scan the entire file. However, they are not as efficient for applications that require frequent searches based on key values.\n\nWhen moving to Azure Cloud, VSAM files are, usually, replicated to Azure SQL Database \n\nVSAM has also many characteristics of a NO-SQL database and than can be moved to Cosmos DB \nhttps://azure.microsoft.com/mediahandler/files/resourcefiles/vsam-to-azure-cosmos-db/VSAM%20to%20Azure%20Cosmos%20DB.pdf) ",
            "Amazon Web Services": "LDS stands for Linear Data Set, which is another type of file organization used in the IBM Virtual Storage Access Method (VSAM) data storage system. Unlike KSDS VSAM files that store data in a sequential order based on a key field, LDS VSAM files store data in a linear, non-sequential order.\n\nIn LDS VSAM files, each record is assigned a Relative Byte Address (RBA) that indicates its position in the file. Records can be accessed sequentially by reading the file in order, or randomly by specifying the RBA of the desired record.\n\nLDS VSAM files are particularly useful for storing large amounts of data that are frequently updated or appended, as they provide fast access to individual records without having to scan the entire file. However, they are not as efficient for applications that require frequent searches based on key values.\n\n\nWhen moving to AWS, VSAM files are, usually, replicated to Amazon RDS\nhttps://docs.aws.amazon.com/prescriptive-guidance/latest/patterns/migrate-and-replicate-vsam-files-to-amazon-rds-or-amazon-msk-using-connect-from-precisely.html\n\n",
            "Gougle Cloud Platform": "LDS stands for Linear Data Set, which is another type of file organization used in the IBM Virtual Storage Access Method (VSAM) data storage system. Unlike KSDS VSAM files that store data in a sequential order based on a key field, LDS VSAM files store data in a linear, non-sequential order.\n\nIn LDS VSAM files, each record is assigned a Relative Byte Address (RBA) that indicates its position in the file. Records can be accessed sequentially by reading the file in order, or randomly by specifying the RBA of the desired record.\n\nLDS VSAM files are particularly useful for storing large amounts of data that are frequently updated or appended, as they provide fast access to individual records without having to scan the entire file. However, they are not as efficient for applications that require frequent searches based on key values."
        },
        "remediations": {},
        "populations": {
            "Mainframe": "(EXISTS {MATCH (o:Object)-[:Property {value:'LDS VSAM File'}]->(op:ObjectProperty)WHERE op.Id IN ['141023']})\n\nFor each LDS VSAM dataset called by Cobol Program: LDS VSAM dataset + Calling Program + ACCESS MODE\nFor each LDS VSAM dataset called by Any Utility: LDS VSAM dataset + Calling Any Utility + ACCESS MODE"
        },
        "description": "Listing CRUD (Create, Read, Update, Delete) operations is important to migrate LDS VSAM files to the cloud because it helps to identify the specific operations that the application performs on the data stored in the VSAM files. \nThis information is critical in determining the appropriate cloud storage solution and configuring it to provide the necessary performance and reliability.\n\n\n\nFor each ESDS VSAM dataset called by Cobol Program: LDS VSAM dataset + Calling Program + ACCESS MODE\nFor each ESDS VSAM dataset called by Any Utility: LDS VSAM dataset + Calling Any Utility + ACCESS MODE",
        "impacts": [
            "replatform"
        ],
        "effort": "high",
        "parents": [
            1202311
        ],
        "outputTable": {
            "template": "inbound-calls-table",
            "table": {
                "head": [
                    "LDS VSAM FILE",
                    "Cobol Program/Paragraph",
                    "ACCESS WITH TYPE"
                ],
                "body": [
                    "",
                    "Caller Cobol Program/Paragraph FullNAME"
                ]
            }
        },
        "outputGraph": {
            "template": "inbound-calls-graph"
        },
        "children": []
    },
    {
        "name": "PDS Files Defined but not storing data",
        "id": 1202048,
        "category": "TASK",
        "branchType": null,
        "rationales": {
            "Agnostic": "Identifying defined Dataset files that are not used for data Storage is useful to optimize migration effort and cost. \nThese datasets may be the symptom of Dead Code and so good candidates to be retired"
        },
        "remediations": {},
        "populations": {
            "Mainframe": "(EXISTS {MATCH (o:Object)-[:Property {value:'PDS Dataset'}]->(op:ObjectProperty)WHERE op.Id IN ['141023']})\n\nPDS dataset Called by Cobol Program with DEFINE link ONLY\nOR PDS dataset called ONLY by JCL Steps\n\n"
        },
        "description": "This rule checks PDS Files defined but not used for data storage.",
        "impacts": [
            "refactor",
            "replatform"
        ],
        "effort": "low",
        "parents": [
            1202305
        ],
        "outputTable": {
            "template": "inbound-calls-table",
            "table": {
                "head": [
                    "PDS FILE",
                    "CALLER FULLNAME",
                    "ACCESS WITH TYPE"
                ],
                "body": [
                    "PDS FULLNAME",
                    "CALLER FULLNAME",
                    "ACCESS WITH TYPE"
                ]
            }
        },
        "outputGraph": {
            "template": "inbound-calls-graph"
        },
        "children": []
    },
    {
        "name": "Cobol Programs Publishing Messages to IBM MQ",
        "id": 1202049,
        "category": "TASK",
        "branchType": null,
        "rationales": {
            "Agnostic": "Identifying COBOL programs that publish messages to queues allows the migration team to assess the scope and complexity of the messaging infrastructure changes required to support these programs in the cloud."
        },
        "remediations": {},
        "populations": {
            "Mainframe": "Cobol Programs publishing Messages to queue  : InternalType= 'CAST_COBOL_SavedProgram' calling a 'CAST_COBOL_MQ_Publisher'"
        },
        "description": "Cobol Programs publishing Messages to queue: Cobol Program calling an IBM MQ Publisher",
        "impacts": [
            "rehost",
            "replatform",
            "review",
            "refactor"
        ],
        "effort": "moderate",
        "parents": [
            1202312
        ],
        "outputTable": {
            "template": "outbound-calls-table",
            "table": {
                "head": [
                    "IBM MQ PUBLISHER",
                    "CALLER FULLNAME",
                    "CALLER OBJECT TYPE"
                ],
                "body": [
                    "Publisher IBM MQ FullNAME",
                    "CALLER Cobol Program/paragraph FullNAME",
                    "Cobol Program/Paragraph Type"
                ]
            }
        },
        "outputGraph": {
            "template": "none"
        },
        "children": []
    },
    {
        "name": "Cobol Programs Subscribing Messages to IBM MQ",
        "id": 1202050,
        "category": "TASK",
        "branchType": null,
        "rationales": {
            "Agnostic": "Identifying COBOL programs that subscribe to an MQ allows the migration team to assess the scope and complexity of the messaging infrastructure changes required to support these programs in the cloud."
        },
        "remediations": {},
        "populations": {
            "Mainframe": "Cobol Programs subscribing to a MQ: InternalType= CAST_COBOL_SavedProgram called by 'CAST_COBOL_MQ_Subscriber'\n"
        },
        "description": "identify Cobol Programs subscribing to an IBM MQ: Cobol Program called by an IBM MQ Subscriber",
        "impacts": [
            "rehost",
            "replatform",
            "review",
            "refactor"
        ],
        "effort": "moderate",
        "parents": [
            1202312
        ],
        "outputTable": {
            "template": "none",
            "table": {
                "head": [
                    "IBM MQ SUBSCRIBER",
                    "CALLEE FullNAME",
                    "CALLEE OBJECT TYPE"
                ],
                "body": [
                    "Subscriber IBM MQ FullNAME",
                    "CALLEE FullNAME",
                    "CALLee OBJECT TYPE"
                ]
            }
        },
        "outputGraph": {
            "template": "outbound-calls-graph"
        },
        "children": []
    },
    {
        "name": "RRDS VSAM Files: Programs/Utilities manipulating Data",
        "id": 1202051,
        "category": "TASK",
        "branchType": null,
        "rationales": {
            "Azure": "RRDS stands for Relative Record Data Set, which is another type of file organization used in the IBM Virtual Storage Access Method (VSAM) data storage system. RRDS VSAM files are used to store data in fixed-length records that can be accessed randomly based on a relative record number (RRN).\n\nIn RRDS VSAM files, each record is assigned a unique RRN that indicates its position in the file. Records can be accessed directly by specifying the RRN of the desired record, which makes RRDS VSAM files particularly useful for applications that require fast random access to individual records.\n\nUnlike KSDS and LDS VSAM files, RRDS VSAM files store records of fixed length, which can be efficient for certain types of data such as binary data. However, RRDS files may not be optimal for applications that store variable-length records or require frequent updates.\n\n\nWhen moving to Azure Cloud, VSAM files are, usually, replicated to Azure SQL Database \n\nVSAM has also many characteristics of a NO-SQL database and than can be moved to Cosmos DB \nhttps://azure.microsoft.com/mediahandler/files/resourcefiles/vsam-to-azure-cosmos-db/VSAM%20to%20Azure%20Cosmos%20DB.pdf) ",
            "Amazon Web Services": "RRDS stands for Relative Record Data Set, which is another type of file organization used in the IBM Virtual Storage Access Method (VSAM) data storage system. RRDS VSAM files are used to store data in fixed-length records that can be accessed randomly based on a relative record number (RRN).\n\nIn RRDS VSAM files, each record is assigned a unique RRN that indicates its position in the file. Records can be accessed directly by specifying the RRN of the desired record, which makes RRDS VSAM files particularly useful for applications that require fast random access to individual records.\n\nUnlike KSDS and LDS VSAM files, RRDS VSAM files store records of fixed length, which can be efficient for certain types of data such as binary data. However, RRDS files may not be optimal for applications that store variable-length records or require frequent updates.\n\nWhen moving to AWS, VSAM files are, usually, replicated to Amazon RDS\nhttps://docs.aws.amazon.com/prescriptive-guidance/latest/patterns/migrate-and-replicate-vsam-files-to-amazon-rds-or-amazon-msk-using-connect-from-precisely.html",
            "Gougle Cloud Platform": "RRDS stands for Relative Record Data Set, which is another type of file organization used in the IBM Virtual Storage Access Method (VSAM) data storage system. RRDS VSAM files are used to store data in fixed-length records that can be accessed randomly based on a relative record number (RRN).\n\nIn RRDS VSAM files, each record is assigned a unique RRN that indicates its position in the file. Records can be accessed directly by specifying the RRN of the desired record, which makes RRDS VSAM files particularly useful for applications that require fast random access to individual records.\n\nUnlike KSDS and LDS VSAM files, RRDS VSAM files store records of fixed length, which can be efficient for certain types of data such as binary data. However, RRDS files may not be optimal for applications that store variable-length records or require frequent updates."
        },
        "remediations": {},
        "populations": {
            "Mainframe": "(EXISTS {MATCH (o:Object)-[:Property {value:'RRDS VSAM File'}]->(op:ObjectProperty)WHERE op.Id IN ['141023']})\n\nFor each RRDS VSAM dataset called by Cobol Program: display the RRDS VSAM dataset + Calling Program + ACCESS MODE\nFor each RRDS VSAM dataset called by Any Utility: display the RRDS VSAM dataset + Calling Utility + ACCESS MODE"
        },
        "description": "Listing CRUD (Create, Read, Update, Delete) operations is important to migrate RRDS VSAM files to the cloud because it helps to identify the specific operations that the application performs on the data stored in the VSAM files. \nThis information is critical in determining the appropriate cloud storage solution and configuring it to provide the necessary performance and reliability.\n\n\nFor each ESDS VSAM dataset called by Cobol Program: RRDS VSAM dataset + Calling Program + ACCESS MODE\nFor each ESDS VSAM dataset called by Any Utility: RRDS VSAM dataset + Calling Any Utility + ACCESS MODE",
        "impacts": [
            "replatform"
        ],
        "effort": "high",
        "parents": [
            1202311
        ],
        "outputTable": {
            "template": "inbound-calls-table",
            "table": {
                "head": [
                    "RRDS VSAM FILE",
                    "Cobol Program/Paragraph",
                    "ACCESS WITH TYPE"
                ],
                "body": [
                    "RRDS VSAM File FullNAME",
                    "Caller Cobol Program/Paragraph FullNAME",
                    "ACCESS WITH TYPE"
                ]
            }
        },
        "outputGraph": {
            "template": "inbound-calls-graph"
        },
        "children": []
    },
    {
        "name": "CICS Green Screens with Multiple Access",
        "id": 1202058,
        "category": "TASK",
        "branchType": null,
        "rationales": {
            "Agnostic": "CICS Screen with multiple access will be complex to review. Each program must be carefully investigated. \n\nPlease, select one of the programs accessing the green screen and we will check if it is connected to the communication or data access layer.\n\n\n\nIf the selected program has linked to MQ system, you will have to expose an API if you plan to have a web app instead of the green screen.\n\nIf the selected program has access to database storage, you will have to expose an API if you plan to have a web app instead of the green screen."
        },
        "remediations": {},
        "populations": {
            "Mainframe": "object_type (CICS_MAP) when the number of calling Cobol program ( Monitor link) > 1 "
        },
        "description": "This rule checks any CICS Map accessed by more than one program ( Monitor link)",
        "impacts": [
            "retire"
        ],
        "effort": "high",
        "parents": [
            1202317
        ],
        "outputTable": {
            "template": "inbound-calls-table",
            "table": {
                "head": [
                    "CICS MAP",
                    "CALLER FULLNAME",
                    "ACCESS WITH TYPE"
                ],
                "body": [
                    "CICS_MAP FULLNAME",
                    "CALLER FULLNAME",
                    "ACCESS WITH TYPE"
                ]
            }
        },
        "outputGraph": {
            "template": "inbound-calls-graph"
        },
        "children": []
    },
    {
        "name": "CICS Green Screens with Single Access",
        "id": 1202063,
        "category": "TASK",
        "branchType": null,
        "rationales": {
            "Agnostic": "Please, select the program accessing the green screen and we will check if it is connected to the communication or data access layer.\n\n\nIf the selected program has linked to MQ system, you will have to expose an API if you plan to have a web app instead of the green screen.\n\nIf the selected program has access to database storage, you will have to expose an API if you plan to have a web app instead of the green screen."
        },
        "remediations": {},
        "populations": {
            "Mainframe": "object_type (CICS_MAP) when the number of calling Cobol program ( Monitor link) = 1"
        },
        "description": "This rule checks any CICS Map, CICS Map definition or CICS Mapset accessed by one single program ( Monitor link)",
        "impacts": [
            "retire"
        ],
        "effort": "moderate",
        "parents": [
            1202317
        ],
        "outputTable": {
            "template": "inbound-calls-table",
            "table": {
                "head": [
                    "CICS MAP",
                    "CALLER FULLNAME",
                    "ACCESS WITH TYPE"
                ],
                "body": [
                    "CICS_MAP FULLNAME",
                    "CALLER FULLNAME",
                    "ACCESS WITH TYPE"
                ]
            }
        },
        "outputGraph": {
            "template": "inbound-calls-graph"
        },
        "children": []
    },
    {
        "name": "CICS DataSet files: Programs/utilities manipulating data",
        "id": 1202064,
        "category": "TASK",
        "branchType": null,
        "rationales": {
            "Agnostic": "CICS allows to access file data in several ways. Most file access is random in the online system because the transactions to be processed are not grouped and sorted in any order. \nTherefore, CICS supports the usual direct access methods - VSAM and DAM (direct access method). It also allows us to access the data using database managers.\n\nWhen mo"
        },
        "remediations": {
            "Azure": "Mainframe green screens not used can be directly retired safely.",
            "Agnostic": "Mainframe green screens not used can be directly retired safely.",
            "Amazon Web Services": "Mainframe green screens not used can be directly retired safely.",
            "Gougle Cloud Platform": "Mainframe green screens not used can be directly retired safely."
        },
        "populations": {
            "Mainframe": "Objects types: CICS_DATASET OR CAST_CICS_DatasetPrototype\n\nEach CICS dataset must be called by the Cobol Program with an ACCESS/reference link"
        },
        "description": "This rule identifies COBOL programs calling CICS Data set",
        "impacts": [
            "retire",
            "review",
            "replatform"
        ],
        "effort": "high",
        "parents": [
            1202311
        ],
        "outputTable": {
            "template": "inbound-calls-table",
            "table": {
                "head": [
                    "CICS DATASET FILE",
                    "Cobol Program/Paragraph",
                    "ACCESS WITH TYPE"
                ],
                "body": [
                    "CICS Dataset File FullNAME",
                    "Caller Cobol Program/Paragraph FullNAME",
                    "ACCESS WITH TYPE"
                ]
            }
        },
        "outputGraph": {
            "template": "inbound-calls-graph"
        },
        "children": []
    },
    {
        "name": "VSAM files Defined but not storing data",
        "id": 1202065,
        "category": "TASK",
        "branchType": null,
        "rationales": {
            "Agnostic": "Identifying defined Datased that are not used for data Storage is useful to optimize migration effort and cost. \nThese datasets may be the symptom of Dead Code and so good candidates to be retired\n"
        },
        "remediations": {},
        "populations": {
            "Mainframe": "VSAM dataset Called by Cobol Program with DEFINE link ONLY\nOR VSAM dataset called ONLY by JCL Steps"
        },
        "description": "This rule checks VSAM datasets defined but not used for data storage.",
        "impacts": [
            "refactor",
            "replatform"
        ],
        "effort": "low",
        "parents": [
            1202311
        ],
        "outputTable": {
            "template": "inbound-calls-table",
            "table": {
                "head": [
                    "VSAM FILE",
                    "CALLER",
                    "ACCESS WITH TYPE"
                ],
                "body": [
                    "VSAM file FullNAME",
                    "CALLER FULLNAME",
                    "ACCESS WITH TYPE"
                ]
            }
        },
        "outputGraph": {
            "template": "inbound-calls-graph"
        },
        "children": []
    },
    {
        "name": "Identify application's adherence to Assembler technology",
        "id": 1202075,
        "category": "TASK",
        "branchType": null,
        "rationales": {
            "Amazon Web Services": "When deploying Mainframe application on Aws EC2 using a Microfocus, note that Assembler executables do not run in production environments.\n\nThe product contains a command line version of MFASM, the mainframe assembler emulator for IBM High Level Assembler programs allows users to create assembler executables or data tables within the Visual Studio environment. \nThe MFASM component is an early adopter release intended to provide a test environment for COBOL applications to call needed assembler subroutines which have already been developed and debugged in other environments. This test environment is for Enterprise Developer only and does not include a debug facility for Assembler. Assembler executables do not run in production environments."
        },
        "remediations": {},
        "populations": {
            "Mainframe": "All objects of Assembler technology  + callers & callees \nInternal types : ASMZOSProgram ( ID= 2043005) OR ASM_MACRO (ID=2043007) OR CallTo_program (ID=\"2043006)"
        },
        "description": "This rule checks all assembler objects and the adherence of the application",
        "impacts": [
            "refactor",
            "rehost"
        ],
        "effort": "moderate",
        "parents": [],
        "outputTable": {
            "template": "none",
            "table": {
                "head": [
                    "OBJECT FULL NAME",
                    "OBJECT TYPE"
                ],
                "body": null
            }
        },
        "outputGraph": {
            "template": "none"
        },
        "children": []
    },
    {
        "name": "Identify application's adherence to Easytrieve technology",
        "id": 1202076,
        "category": "TASK",
        "branchType": null,
        "rationales": {
            "Amazon Web Services": "Easytrieve is supported by MicroFocus Contact your Micro Focus SupportLine representative for more details \nhttps://www.microfocus.com/documentation/enterprise-developer/ed30/Upgrading_to_ED_for_VS_30.pdf"
        },
        "remediations": {},
        "populations": {
            "Mainframe": "All objects of Easytreive technology  + callers & callees \nInternal types : Eztprogram OR Easymacro OR Easyfile OR Easyproc OR Easyreport"
        },
        "description": "This rule checks all Easytreive objects and the adherence to the application",
        "impacts": [
            "refactor",
            "rehost"
        ],
        "effort": "low",
        "parents": [],
        "outputTable": {
            "template": "none",
            "table": {
                "head": [
                    "OBJECT FULL NAME",
                    "OBJECT TYPE"
                ],
                "body": null
            }
        },
        "outputGraph": {
            "template": "none"
        },
        "children": []
    },
    {
        "name": "Identify application's adherence to Assembler technology",
        "id": 1202078,
        "category": "TASK",
        "branchType": null,
        "rationales": {
            "Agnostic": "The presence of the Assembler code influences the migration strategy. Assembler programs may have unique considerations during modernization, such as conversion to a higher-level language, rewriting, or reengineering. Understanding the extent of Assembler code allows you to design an appropriate migration plan that addresses these specific requirements.\n\nFor the conversion of the code, the presence of the assembler technology will also guide the choice of the partner and tool as the conversion of the assembler is not systematically supported"
        },
        "remediations": {},
        "populations": {
            "Mainframe": "All objects of Assembler technology  + callers & callees \nInternal types : ASMZOSProgram ( ID= 2043005) OR ASM_MACRO (ID=2043007) OR CallTo_program (ID=\"2043006)"
        },
        "description": "This rule checks all assembler objects and the adherence to the application",
        "impacts": [
            "refactor",
            "rehost"
        ],
        "effort": "high",
        "parents": [
            1202304
        ],
        "outputTable": {
            "template": "none",
            "table": {
                "head": [
                    "OBJECT FULL NAME",
                    "OBJECT TYPE",
                    "LOC"
                ],
                "body": null
            }
        },
        "outputGraph": {
            "template": "none"
        },
        "children": []
    },
    {
        "name": "Identify application's adherence to Easytrieve technology",
        "id": 1202079,
        "category": "TASK",
        "branchType": null,
        "rationales": {
            "Agnostic": "If you have Easytrieve code it will have an impact on your migration strategy. Easytrieve Technology may has unique considerations during modernization, such as conversion to a higher-level language, rewriting, or reengineering. Understanding the extent of Easytrieve code allows you to design an appropriate migration plan that addresses these specific requirements.\n\nFor the conversion of the code, the presence of the Easytrieve technology will also guide the choice of the partner and tool as the conversion of the Easytrieve is not systematically supported"
        },
        "remediations": {},
        "populations": {
            "Mainframe": "All objects of Easytrieve technology  + callers & callees \nInternal types : Eztprogram OR Easymacro OR Easyfile OR Easyproc OR Easyreport"
        },
        "description": "This rule checks all Easytrieve objects and the adherence to the application",
        "impacts": [
            "refactor",
            "rehost"
        ],
        "effort": "high",
        "parents": [
            1202304
        ],
        "outputTable": {
            "template": "none",
            "table": {
                "head": [
                    "OBJECT FULL NAME",
                    "OBJECT TYPE",
                    "LOC"
                ],
                "body": null
            }
        },
        "outputGraph": {
            "template": "none"
        },
        "children": []
    },
    {
        "name": "Identify application's adherence to Rexx technology",
        "id": 1202080,
        "category": "TASK",
        "branchType": null,
        "rationales": {
            "Agnostic": "The presence of the Rexx code influences the migration strategy. Rexx Technology may has unique considerations during modernization, such as conversion to a higher-level language, rewriting, or reengineering. Understanding the extent of Easytreive code allows you to design an appropriate migration plan that addresses these specific requirements.\n\nFor the conversion of the code, the presence of the Rexx technology will also guide the choice of the partner and tool as the conversion of Rexx is not systematically supported"
        },
        "remediations": {},
        "populations": {
            "Mainframe": "All objects of Rexx technology  + callers & callees \nInternal types : Rexxprogram OR Rexxfile OR Rexxfunction OR Rexxprocedure OR Unknown_Rexxprogram"
        },
        "description": "This rule checks all Rexx objects and the adherence to the application",
        "impacts": [
            "refactor",
            "rehost"
        ],
        "effort": "high",
        "parents": [
            1202304
        ],
        "outputTable": {
            "template": "none",
            "table": {
                "head": [
                    "OBJECT FULL NAME",
                    "OBJECT TYPE",
                    "LOC"
                ],
                "body": null
            }
        },
        "outputGraph": {
            "template": "none"
        },
        "children": []
    },
    {
        "name": "Cobol Program accessing IMS DB Segment",
        "id": 1202085,
        "category": "TASK",
        "branchType": null,
        "rationales": {
            "Agnostic": "Identifying IMS segments, their access mode, and eventually their relationships can help in understanding the design of the appropriate relational database schema.\n\nCobol programs that access IMS-DB will need to be modified to interact with the new relational database. By identifying the programs and their dependencies on IMS-DB, you can plan to implement the necessary changes in the application code, such as updating SQL queries or data access logic."
        },
        "remediations": {},
        "populations": {
            "Mainframe": "IMS Segment (InternalType = CAST_IMS_AnalyzedSegment)\nFor Each Segment, provide the PCB calling  (InternalType: CAST_IMS_DatabaseProgramControlBlock) + Calling Cobol Program\n\n"
        },
        "description": "This rule identifies the IMS Segments used in the application and Progams accessing these segments via PCB/PSB",
        "impacts": [],
        "effort": "high",
        "parents": [
            1202289
        ],
        "outputTable": {
            "template": "none",
            "table": {
                "head": [
                    "IMS SEGMENT",
                    "IMS PCB",
                    "COBOL PROGRAM/PARAGRAPPAH",
                    "ACCESS WITH TYPE"
                ],
                "body": [
                    "IMS SEGMENT  FULLNAME",
                    "CALLER FULLNAME: CAST_IMS_DatabaseProgramControlBlock",
                    "CALLER FULLNAME: Cobol Program/section/paragraph",
                    "Link type ( from Cobol to IMS DB PCB)"
                ]
            }
        },
        "outputGraph": {
            "template": "none"
        },
        "children": []
    },
    {
        "name": "Missing IMS Segment",
        "id": 1202086,
        "category": "TASK",
        "branchType": null,
        "rationales": {
            "Agnostic": "Identifying IMS segments, their access mode and eventually their relationships can help understanding the design of the appropriate relational database schema.\n\n\nCobol programs that access IMS DB will need to be modified to interact with the new relational database. By identifying the programs and their dependencies on IMS DB, you can plan and implement the necessary changes in the application code, such as updating SQL queries or data access logic.\n\n\n\n\n"
        },
        "remediations": {},
        "populations": {
            "Mainframe": "IMS Segment (InternalType: CAST_IMS_SegmentPrototype ) + Caller object \n\n"
        },
        "description": "This rule identifies Missing IMS Segments that have been identified in the application",
        "impacts": [],
        "effort": "moderate",
        "parents": [
            1202289
        ],
        "outputTable": {
            "template": "missing-objects-table",
            "table": {
                "head": [
                    "MISSING IMS SEGMENT",
                    "CALLER FULLNAME",
                    "CALLER TYPE "
                ],
                "body": [
                    "CAST_IMS_SegmentPrototyp Fullname",
                    "CALLER  FullNAME",
                    "CALLER OBJECT TYPE",
                    ""
                ]
            }
        },
        "outputGraph": {
            "template": "violations-graph"
        },
        "children": []
    },
    {
        "name": "Missing IMS PSB, IMS PCB and IMS DB Definition .",
        "id": 1202087,
        "category": "TASK",
        "branchType": null,
        "rationales": {
            "Agnostic": "Missing IMS PSB, IMS PCB and IMS DB Definition should be identified before migration.  \n"
        },
        "remediations": {},
        "populations": {
            "Mainframe": "Unknnow IMS PSB: CAST_IMS_ProgramSpecificationBlockPrototype \nOR Unknnow IMS PCB: CAST_IMS_ProgramControlBlockPrototype\nOR Unknnow IM DB Definition: CAST_IMS_DatabasePrototype\n\n\nAND ALL JCL Step, JCL Procedure, and Cobol Program referencing them \n\n\n"
        },
        "description": "This rule identifies all Missing IMS PSB, Missing IM PCB, Missing IMS DB Definition and All JCL step, procedure and Cobol Program referencing them",
        "impacts": [
            "refactor"
        ],
        "effort": "high",
        "parents": [
            1202289
        ],
        "outputTable": {
            "template": "missing-objects-table",
            "table": {
                "head": [
                    "MISSING OBJECT FULLNAME",
                    "CALLER FULLNAME",
                    "CALLER OBJECT-TYPE"
                ],
                "body": [
                    "CAST_IMS_ProgramSpecificationBlockPrototype OR CAST_IMS_ProgramControlBlockPrototype OR\nCAST_IMS_DatabasePrototype",
                    "CALLER FULLNAME",
                    "CALLER OBJECT-TYPE"
                ]
            }
        },
        "outputGraph": {
            "template": "missing-objects-graph"
        },
        "children": []
    },
    {
        "name": "IMS GSAM Files:  Programs/Utilities manipulating Data",
        "id": 1202093,
        "category": "TASK",
        "branchType": null,
        "rationales": {
            "Agnostic": "Identifying IMS GSAM, their access mode and eventually their relationships can help understanding the design of the appropriate relational database schema.\n\n\nCobol programs that access IMS GSAM will need to be modified to interact with the new relational database. By identifying the programs and their dependencies on IMS GSAM Datasets, you can plan and implement the necessary changes in the application code, such as updating SQL queries or data access logic."
        },
        "remediations": {},
        "populations": {
            "Mainframe": "GSAM DataSet + Caller : CAST_IMS_GSAMProgramControlBlock + Caller Cobol Program\n\n\n"
        },
        "description": "This rule identifies the IMS GSAM DataSet used in the application and Progams accessing these Datasets.",
        "impacts": [],
        "effort": "high",
        "parents": [
            1202311
        ],
        "outputTable": {
            "template": "none",
            "table": {
                "head": [
                    "GSAM FILE",
                    "PCB",
                    "COBOL PARAGRAPH",
                    "ACCESS WITH TYPE"
                ],
                "body": [
                    "GSAM DataSet  FULLNAME",
                    "CALLER FULLNAME: CAST_IMS_GSAMProgramControlBlock",
                    "CALLER FULLNAME: Cobol Program/section/paragraph",
                    "Link type ( from Cobol to IMS GSAM PCB)"
                ]
            }
        },
        "outputGraph": {
            "template": "none"
        },
        "children": []
    },
    {
        "name": "Identify application's adherence to Focus technology",
        "id": 1202095,
        "category": "TASK",
        "branchType": null,
        "rationales": {
            "Agnostic": "The presence of the Focus code influences the migration strategy. Focus Technology may has unique considerations during modernization, such as conversion to a higher-level language, rewriting, or reengineering. Understanding the extent of Focus code allows you to design an appropriate migration plan that addresses these specific requirements.\n\nFor the conversion of the code, the presence of the Focus technology will also guide the choice of the partner and tool as the conversion of Focus is not systematically supported"
        },
        "remediations": {},
        "populations": {
            "Mainframe": "All objects of Focus technology  + callers & callees \nInternal types : FOCUSTable OR FOCUSTablef OR FOCUSDefine OR FOCUSMatch OR FOCUSGraph OR FOCUSJoin OR FOCUSMaster OR FOCUSprocedure"
        },
        "description": "This rule checks all Focus objects and the adherence to the application",
        "impacts": [
            "refactor",
            "rehost"
        ],
        "effort": "high",
        "parents": [
            1202304
        ],
        "outputTable": {
            "template": "none",
            "table": {
                "head": [
                    "OBJECT FULL NAME",
                    "OBJECT TYPE",
                    "LOC"
                ],
                "body": null
            }
        },
        "outputGraph": {
            "template": "none"
        },
        "children": []
    },
    {
        "name": "Identify application's adherence to Rexx technology",
        "id": 1202097,
        "category": "TASK",
        "branchType": null,
        "rationales": {
            "Amazon Web Services": "The presence of the Rexx code influences the migration strategy. Rexx Technology may has unique considerations during modernization, such as conversion to a higher-level language, rewriting, or reengineering. Understanding the extent of Rexx code allows you to design an appropriate migration plan that addresses these specific requirements.\n\nFor the conversion of the code, the presence of the Rexx technology will also guide the choice of the partner and tool as the conversion of Rexx is not systematically supported"
        },
        "remediations": {},
        "populations": {
            "Mainframe": "All objects of Rexx technology  + callers & callees \nInternal types : Rexxprogram OR Rexxfile OR Rexxfunction OR Rexxprocedure OR Unknown_Rexxprogram"
        },
        "description": "This rule checks all Rexx objects and the adherence to the application",
        "impacts": [
            "refactor",
            "rehost"
        ],
        "effort": "moderate",
        "parents": [],
        "outputTable": {
            "template": "none",
            "table": {
                "head": [
                    "OBJECT FULL NAME",
                    "OBJECT TYPE"
                ],
                "body": null
            }
        },
        "outputGraph": {
            "template": "none"
        },
        "children": []
    },
    {
        "name": "Identify application's adherence to Focus technology",
        "id": 1202098,
        "category": "TASK",
        "branchType": null,
        "rationales": {
            "Amazon Web Services": "The presence of the Focus code influences the migration strategy. Focus Technology may has unique considerations during modernization, such as conversion to a higher-level language, rewriting, or reengineering. Understanding the extent of Focus code allows you to design an appropriate migration plan that addresses these specific requirements.\n\nFor the conversion of the code, the presence of the Focus technology will also guide the choice of the partner and tool as the conversion of Focus is not systematically supported"
        },
        "remediations": {},
        "populations": {
            "Mainframe": "All objects of Focus technology  + callers & callees \nInternal types : FOCUSTable OR FOCUSTablef OR FOCUSDefine OR FOCUSMatch OR FOCUSGraph OR FOCUSJoin OR FOCUSMaster OR FOCUSprocedure"
        },
        "description": "This rule checks all Focus objects and the adherence to the application",
        "impacts": [
            "refactor",
            "rehost"
        ],
        "effort": "moderate",
        "parents": [],
        "outputTable": {
            "template": "none",
            "table": {
                "head": [
                    "OBJECT FULL NAME",
                    "OBJECT TYPE"
                ],
                "body": null
            }
        },
        "outputGraph": {
            "template": "none"
        },
        "children": []
    },
    {
        "name": "CICS transactions triggering Cobol Program",
        "id": 1202148,
        "category": "TASK",
        "branchType": null,
        "rationales": {
            "Agnostic": "Identifying and analyzing the existing CICS transactions allows the migration team to assess the scope and complexity of the replacement Entry points in used to start a CICS transaction and run associated programs.\n\nTransactional programs will require code changes to handle web communication\n\nNote also that, transactional programs are good candidates to be refactored and transformed into Lambda function.\n\n"
        },
        "remediations": {},
        "populations": {
            "Mainframe": "CICS_TRANSACTION Calling Cobol Program"
        },
        "description": "This rule identifies COBOL programs called by COBOL Transactions",
        "impacts": [
            "retire",
            "review"
        ],
        "effort": "low",
        "parents": [
            1202316
        ],
        "outputTable": {
            "template": "none",
            "table": {
                "head": [
                    "CICS TRANSACTION",
                    "CALLEE FULLNAME",
                    "CALLEE TYPE"
                ],
                "body": null
            }
        },
        "outputGraph": {
            "template": "outbound-calls-graph"
        },
        "children": []
    },
    {
        "name": "Missing CICS Transactions",
        "id": 1202149,
        "category": "TASK",
        "branchType": null,
        "rationales": {
            "Agnostic": "Missing CICS Transactions should be identified before migration as should be part of the migration scope."
        },
        "remediations": {},
        "populations": {
            "Mainframe": "CAST_CICS_TransactionPrototype and called Program\n"
        },
        "description": "This rule identifies Missing Transactions and called programs",
        "impacts": [
            "refactor"
        ],
        "effort": null,
        "parents": [
            1202316
        ],
        "outputTable": {
            "template": "missing-objects-table",
            "table": {
                "head": [
                    "MISSING CICS TRANSACTION",
                    "CALLER FULLNAME",
                    "CALLER OBJECT TYPE"
                ],
                "body": null
            }
        },
        "outputGraph": {
            "template": "missing-objects-graph"
        },
        "children": []
    },
    {
        "name": "Cobol Program using CICS Transient data",
        "id": 1202150,
        "category": "TASK",
        "branchType": null,
        "rationales": {
            "Agnostic": "Identifying CICS Transient data allows the migration team to assess the scope of the required changes and the complexity of integrating the new Cloud replacement system.\n\n\nThe Cobol Program accessing the CICS transient Data Queue service need also to be identified. These programs will require a change/review of the connection APIs with the new cloud messaging service "
        },
        "remediations": {},
        "populations": {
            "Mainframe": "CAST_CICS_TDQueue and calling Program "
        },
        "description": "This rule identifies COBOL programs Calling CICS Transient Data",
        "impacts": [
            "retire",
            "review"
        ],
        "effort": "low",
        "parents": [
            1202314
        ],
        "outputTable": {
            "template": "inbound-calls-table",
            "table": {
                "head": [
                    "CICS TRANSIENT DATA FULLNAME",
                    "CALLER FULLNAME",
                    "ACCESS WITH TYPE"
                ],
                "body": null
            }
        },
        "outputGraph": {
            "template": "inbound-calls-graph"
        },
        "children": []
    },
    {
        "name": "Missing CICS Transient Data",
        "id": 1202151,
        "category": "TASK",
        "branchType": null,
        "rationales": {
            "Agnostic": "Missing CIS Transient Data should be identified before migration as should be part of the migration scope.\n\n\n"
        },
        "remediations": {},
        "populations": {
            "Mainframe": "CAST_CICS_TDQueuePrototype and calling Program"
        },
        "description": "This rule identifies Missing CICS Transient Data",
        "impacts": [],
        "effort": null,
        "parents": [
            1202314
        ],
        "outputTable": {
            "template": "missing-objects-table",
            "table": {
                "head": [
                    "CICS Transient Data ",
                    "CALLER FULLNAME",
                    "CALLER OBJECT TYPE"
                ],
                "body": [
                    "CAST_CICS_TDQUEUEPROTOTYPE FULLLNAME",
                    "CALLER FULLNAME",
                    "CALLER OBJECT TYPE"
                ]
            }
        },
        "outputGraph": {
            "template": "missing-objects-graph"
        },
        "children": []
    },
    {
        "name": "Cobol Programs candidate for Cloud function conversion : No Data Access",
        "id": 1202181,
        "category": "TASK",
        "branchType": null,
        "rationales": {
            "Azure": "Cobol Programs not accessing any DB table are good candidates for Azure Function conversion",
            "Amazon Web Services": "Cobol Programs not accessing any DB table not Data file are good candidates for AWS Lambda conversion"
        },
        "remediations": {},
        "populations": {
            "Mainframe": "CAST_COBOL_SavedProgram with the property 'Number of Transaction' = 1\nThe Entry Point of the transaction should have the property 'Number of Transaction' = 1\n\nThe program is NOT accessing any Database table or NOT Shared DataSet File Storage\n\n"
        },
        "description": "This rule identifies the program candidates for AWS Lambda function conversion Not accessing Data Storage object",
        "impacts": [],
        "effort": "low",
        "parents": [
            1202300
        ],
        "outputTable": {
            "template": "none",
            "table": {
                "head": [
                    "COBOL PROGRAM FULLNAME",
                    "TRANSACTION NAME"
                ],
                "body": null
            }
        },
        "outputGraph": {
            "template": "none"
        },
        "children": []
    },
    {
        "name": "Cobol Program monitoring CICS Green Screens",
        "id": 1202211,
        "category": "TASK",
        "branchType": null,
        "rationales": {
            "Azure": "Identifying Transactional Programs Monitoring CICS screens is important to evaluate the scope of changes required before the integration into the new Azure platform. ",
            "Amazon Web Services": "Identifying Transactional Programs Monitoring CICS screens is important since specific changes will be required as code changes will be necessary for Screens and Transactional programs before the integration into the new AWS platform. "
        },
        "remediations": {},
        "populations": {
            "Mainframe": "search for nodes with type is one of (COBOL_CICS_MAP OR CICS_MAP) with calling Programs"
        },
        "description": "This rule lists CICS Maps in your application and the COBOL programs/paragraphs Monitoring them",
        "impacts": [
            "rehost"
        ],
        "effort": "low",
        "parents": [
            1202297
        ],
        "outputTable": {
            "template": "inbound-calls-table",
            "table": {
                "head": [
                    "CICS MAP FULLNAME",
                    "COBOL PROGRAM/PARAGRAPH",
                    "ACCESS WITH TYPE"
                ],
                "body": [
                    "CICS MAP FullNAME",
                    "CALLER( Cobol Program/PARAGRAPH) FullNAME",
                    "ACCESS WITH TYPE"
                ]
            }
        },
        "outputGraph": {
            "template": "inbound-calls-graph"
        },
        "children": []
    },
    {
        "name": "Missing CICS Green Screens",
        "id": 1202212,
        "category": "TASK",
        "branchType": null,
        "rationales": {
            "Agnostic": "Missing CICS Map should be identified before migration. "
        },
        "remediations": {},
        "populations": {
            "Mainframe": "CAST_CICS_MapPrototype and Caller Cobol Program "
        },
        "description": "This Rule provides a list of missing CICS MAP and Cobol programs/paragraphs referencing them",
        "impacts": [
            "rehost"
        ],
        "effort": "low",
        "parents": [
            1202297
        ],
        "outputTable": {
            "template": "missing-objects-table",
            "table": {
                "head": [
                    "CICS MISSING MAP",
                    "CALLER FULLNAME",
                    "CALLER OBJECT-TYPE"
                ],
                "body": [
                    "CAST_CICS_MapPrototype FullNAME",
                    "CALLER FullNAME",
                    "CALLER OBJECT TYPE"
                ]
            }
        },
        "outputGraph": {
            "template": "missing-objects-graph"
        },
        "children": []
    },
    {
        "name": "Cobol Program using CICS transactions",
        "id": 1202214,
        "category": "TASK",
        "branchType": null,
        "rationales": {
            "Azure": "Analyzing the existing CIS applications and identifying CICS Transactions used in the application and their interaction with different programs is a very important step before starting Migration. \n\nThis allows you to evaluate the required adjustments that address the specification of the chosen emulation solution provider to ensure compatibility, performance, and ongoing support for your specific use case\n\nNote also that CICS Transactional programs will require code change to handle the integration of the emulation solution",
            "Amazon Web Services": "Identifying Cobol Programs processing transactions is important to evaluate the scope of changes required before the integration into the new AWS platform. "
        },
        "remediations": {},
        "populations": {
            "Mainframe": "CICS_TRANSACTION and called Program"
        },
        "description": "This rule identifies CICS Transactions processed in mainframe applications within CICS environment and called Program",
        "impacts": [
            "rehost"
        ],
        "effort": "low",
        "parents": [
            1202298
        ],
        "outputTable": {
            "template": "outbound-calls-table",
            "table": {
                "head": [
                    "CICS TRANSACTION",
                    "CALLEE OBJECT",
                    "CALLEE TYPE"
                ],
                "body": [
                    "CICS TRansaction",
                    "CALLEE FULLNAME ( Cobol Program)",
                    "CALLEE TYPE"
                ]
            }
        },
        "outputGraph": {
            "template": "inbound-calls-graph"
        },
        "children": []
    },
    {
        "name": "Mssing CICS Transactions",
        "id": 1202215,
        "category": "TASK",
        "branchType": null,
        "rationales": {
            "Agnostic": "Missing CICS Transaction should be identified before migration. "
        },
        "remediations": {},
        "populations": {
            "Mainframe": "CAST_CICS_TransactionPrototype and called Program\n"
        },
        "description": "This rule identifies unknown COBOL Transactions and called programs",
        "impacts": [
            "rehost"
        ],
        "effort": "low",
        "parents": [
            1202298
        ],
        "outputTable": {
            "template": "missing-objects-table",
            "table": {
                "head": [
                    "MISSING TRANSACTION",
                    "CALLER OBJECT",
                    "CALLER OBJECT TYPE"
                ],
                "body": [
                    "CAST_CICS_TransactionPrototype",
                    "CALLER FULLNAME",
                    "CALLER OBJECT TYPE"
                ]
            }
        },
        "outputGraph": {
            "template": "missing-objects-graph"
        },
        "children": []
    },
    {
        "name": "Cobol Program using CICS Transient data",
        "id": 1202217,
        "category": "TASK",
        "branchType": null,
        "rationales": {
            "Agnostic": "Identifying CICS transient data and all Cobol Programs using transient Data is important since many changes will be required to integrate the replication services"
        },
        "remediations": {},
        "populations": {
            "Mainframe": "CAST_CICS_TDQueue and calling Program "
        },
        "description": "This rule identifies COBOL programs Calling CICS Transient Data",
        "impacts": [
            "review",
            "rehost"
        ],
        "effort": "low",
        "parents": [
            1202299
        ],
        "outputTable": {
            "template": "inbound-calls-table",
            "table": {
                "head": [
                    "CICS TRANSIENT DATA",
                    "CALLER FULLNAMECOBOL PROGRAM/PARAGRAPH",
                    "ACCESS WITH TYPE"
                ],
                "body": [
                    "CICS Transient Data",
                    "CALLER FULLNAME(Cobol Program/Paragraph)",
                    "ACCESS WITH TYPE"
                ]
            }
        },
        "outputGraph": {
            "template": "inbound-calls-graph"
        },
        "children": []
    },
    {
        "name": "Missing Transient Data",
        "id": 1202218,
        "category": "TASK",
        "branchType": null,
        "rationales": {
            "Agnostic": "Missing CICS Transient Data should be identified before migration. "
        },
        "remediations": {},
        "populations": {
            "Mainframe": "CAST_CICS_TDQueuePrototype and cobol programs calling Program"
        },
        "description": "This rule identifies Unknown CICS Transient Data",
        "impacts": [
            "rehost"
        ],
        "effort": "low",
        "parents": [
            1202299
        ],
        "outputTable": {
            "template": "missing-objects-table",
            "table": {
                "head": [
                    "MISSING TRANSIENT DATA",
                    "CALLER FULLNAME",
                    "CALLER OBJECT TYPE"
                ],
                "body": [
                    "Missing TRansient Data",
                    "CALLER FULLNAME",
                    "CALLER OBJECT TYPE"
                ]
            }
        },
        "outputGraph": {
            "template": "missing-objects-graph"
        },
        "children": []
    },
    {
        "name": "Message Input Descriptor Triggering Cobol Program",
        "id": 1202262,
        "category": "TASK",
        "branchType": null,
        "rationales": {
            "Agnostic": "Identifying Device Inpit Format of Message Format Services and their interaction with different programs is a very important step before starting Migration. \n\n"
        },
        "remediations": {},
        "populations": {
            "Mainframe": "CAST_IMS_MessageInputDescriptor Calling COBOL Program "
        },
        "description": "This rule identifies Message Input Descriptors calling Cobol Program",
        "impacts": [
            "rehost"
        ],
        "effort": "low",
        "parents": [
            1202261
        ],
        "outputTable": {
            "template": "none",
            "table": {
                "head": [
                    "IMS MID FULLNAME",
                    "CALLEE FULLNAME",
                    "CALLEE OBJECT-TYPE"
                ],
                "body": [
                    "CAST_IMS_MessageInputDescriptor FullNAME",
                    "CALLEE FULLNAME ",
                    "CALLEE OBJECT-TYPE"
                ]
            }
        },
        "outputGraph": {
            "template": "outbound-calls-graph"
        },
        "children": []
    },
    {
        "name": "Message Input Descriptor and Message Output Descriptor not used",
        "id": 1202263,
        "category": "TASK",
        "branchType": null,
        "rationales": {
            "Agnostic": "If the selected program has access to database storage, you will have to expose an API if you plan to have a web app instead of the green screen."
        },
        "remediations": {},
        "populations": {
            "Mainframe": "CAST_IMS_MessageFormatService, CAST_IMS_MessageInputDescriptor, CAST_IMS_MessageOutputDescriptor with Number of Linked Objects = 0"
        },
        "description": "This rule checks any IMS Message Format Service, IMS Message Input Descriptor or IMS Message Output Descriptor not linked to any Cobol Program",
        "impacts": [
            "rehost"
        ],
        "effort": "low",
        "parents": [
            1202261,
            1202278
        ],
        "outputTable": {
            "template": "none",
            "table": {
                "head": [
                    "OBJECT FULLNAME",
                    "OBJECT-TYPE"
                ],
                "body": null
            }
        },
        "outputGraph": {
            "template": "none"
        },
        "children": []
    },
    {
        "name": "IMS Transactions Triggering Cobol Program",
        "id": 1202271,
        "category": "TASK",
        "branchType": null,
        "rationales": {
            "Agnostic": "Identifying and analyzing the existing IMS transactions allows the migration team to assess the scope and complexity of changes required to expose web services.\nNote also that IMS programs interacting with IMS transactions will require code changes to handle web service communication\n"
        },
        "remediations": {},
        "populations": {
            "Mainframe": "CAST_IMS_Transaction Calling Cobol Paragraph/program."
        },
        "description": "This rule identifies IMS Transactions Calling COBOL programs",
        "impacts": [
            "rehost"
        ],
        "effort": "low",
        "parents": [
            1202257,
            1202320
        ],
        "outputTable": {
            "template": "none",
            "table": {
                "head": [
                    "IMS TRANSACTION",
                    "CALLEE FULLNAME",
                    "CALLEE OBJECT TYPE"
                ],
                "body": [
                    "CAST_IMS_Transaction FULLNAME",
                    "CALLEE FUllNAME",
                    "CALLEE OBJECT TYPE"
                ]
            }
        },
        "outputGraph": {
            "template": "outbound-calls-graph"
        },
        "children": []
    },
    {
        "name": "IMS transactions Triggered by Cobol Program",
        "id": 1202272,
        "category": "TASK",
        "branchType": null,
        "rationales": {
            "Agnostic": "Identifying and analyzing the existing IMS transactions allows the migration team to assess the scope and complexity of changes required to expose web services.\nNote also that IMS programs interacting with IMS transactions will require code changes to handle web service communication\n"
        },
        "remediations": {},
        "populations": {
            "Mainframe": "CAST_IMS_Transaction Called by a Cobol Paragraph."
        },
        "description": "This rule identifies IMS Transactions Called by COBOL programs",
        "impacts": [
            "rehost"
        ],
        "effort": "low",
        "parents": [
            1202257,
            1202320
        ],
        "outputTable": {
            "template": "outbound-calls-table",
            "table": {
                "head": [
                    "IMS TRANSACTION",
                    "CALLER FULLNAME",
                    "CALLER OBJECT TYPE"
                ],
                "body": [
                    "CAST_IMS_Transaction FUllNAME",
                    "CALLER Cobol program",
                    "CALLER OBJECT TYPE"
                ]
            }
        },
        "outputGraph": {
            "template": "none"
        },
        "children": []
    },
    {
        "name": "Message Out Descriptor Triggered by Cobol Program",
        "id": 1202275,
        "category": "TASK",
        "branchType": null,
        "rationales": {
            "Agnostic": "Identifying Device Inpit Format of Message Format Services and their interaction with different programs is a very important step before starting Migration. \n"
        },
        "remediations": {},
        "populations": {
            "Mainframe": "CAST_IMS_MessageOutputDescriptor Called by Cobol Program "
        },
        "description": "This rule identifies IMS Message Output Descriptor in mainframe applications  and called Program",
        "impacts": [
            "rehost"
        ],
        "effort": "low",
        "parents": [
            1202261
        ],
        "outputTable": {
            "template": "outbound-calls-table",
            "table": {
                "head": [
                    "IMS MOD FULLNAME",
                    "CALLER FULLNAME",
                    "CALLER OBJECT TYPE"
                ],
                "body": [
                    "CAST_IMS_MessageOutputDescriptor FULLNAME"
                ]
            }
        },
        "outputGraph": {
            "template": "outbound-calls-graph"
        },
        "children": []
    },
    {
        "name": "Missing CICS Green Screens",
        "id": 1202276,
        "category": "TASK",
        "branchType": null,
        "rationales": {
            "Agnostic": "Missing CICS BMS Screns should be identified before migration as should be part of the migration scope"
        },
        "remediations": {},
        "populations": {
            "Mainframe": "CAST_CICS_MapPrototype"
        },
        "description": "",
        "impacts": [],
        "effort": null,
        "parents": [
            1202317
        ],
        "outputTable": {
            "template": "missing-objects-table",
            "table": {
                "head": [
                    "MISSING CICS MAP",
                    "CALLER FULLNAME",
                    "ACCESS WITH TYPE"
                ],
                "body": [
                    "CAST_CICS_MapPrototype",
                    "CALLER FULLNAME",
                    "CALLER OBJECT TYPE"
                ]
            }
        },
        "outputGraph": {
            "template": "missing-objects-graph"
        },
        "children": []
    },
    {
        "name": "Message Input Descriptor Triggering Multiple Cobol Programs",
        "id": 1202279,
        "category": "TASK",
        "branchType": null,
        "rationales": {
            "Agnostic": "Message Input Descriptor with multiple access will be complex to review or replace \nEach Cobol program Called by the MID must be carefully investigated.\nWill be also recommended to check if the same program is connected to a communication or data access layer.\n\n\n"
        },
        "remediations": {},
        "populations": {
            "Mainframe": "CAST_IMS_MessageInputDescriptor Calling COBOL Program And Number of CALLEE COBOL Program >1\n\n"
        },
        "description": "This rule identifies Message Input Descriptors calling Multiple Cobol Programs",
        "impacts": [
            "rehost"
        ],
        "effort": "low",
        "parents": [
            1202278
        ],
        "outputTable": {
            "template": "none",
            "table": {
                "head": [
                    "IMS MID FULLNAME",
                    "CALLEE FULLNAME",
                    "CALLEE OBJECT-TYPE"
                ],
                "body": null
            }
        },
        "outputGraph": {
            "template": "none"
        },
        "children": []
    },
    {
        "name": "Message Input Descriptor Triggering One Single Cobol Program",
        "id": 1202280,
        "category": "TASK",
        "branchType": null,
        "rationales": {
            "Agnostic": "Message Input Descriptors with single access are less complex to review or replace \nEach Cobol program Called by the MID must be carefully investigated.\nWill be also recommended to check if the same program is connected to a communication or data access layer.\n\n\n"
        },
        "remediations": {},
        "populations": {
            "Mainframe": "CAST_IMS_MessageInputDescriptor Calling COBOL Program And Number of CALLEE COBOL Program =1"
        },
        "description": "This rule identifies Message Input Descriptors calling Cobol Program",
        "impacts": [
            "rehost"
        ],
        "effort": "low",
        "parents": [
            1202278
        ],
        "outputTable": {
            "template": "none",
            "table": {
                "head": [
                    "IMS MID FULLNAME",
                    "CALLEE FULLNAME",
                    "CALLEE OBJECT-TYPE"
                ],
                "body": null
            }
        },
        "outputGraph": {
            "template": "none"
        },
        "children": []
    },
    {
        "name": "Message Out Descriptor Triggered by Multiple Cobol Programs",
        "id": 1202281,
        "category": "TASK",
        "branchType": null,
        "rationales": {
            "Agnostic": "Message Output Descriptor with multiple access will be complex to review or replace \nEach Cobol program Calling the MOD must be carefully investigated.\nWill be also recommended to check if the same program is connected to a communication or data access layer.\n\n\n\n"
        },
        "remediations": {},
        "populations": {
            "Mainframe": "CAST_IMS_MessageOutputDescriptor Called by Cobol Program And Number of CALLER COBOL Program >1"
        },
        "description": "This rule identifies IMS Message Output Descriptor in mainframe applications Called by Multiple Cobol Programs",
        "impacts": [
            "rehost"
        ],
        "effort": "low",
        "parents": [
            1202278
        ],
        "outputTable": {
            "template": "outbound-calls-table",
            "table": {
                "head": [
                    "IMS MOD FULLNAME",
                    "CALLER FULLNAME",
                    "CALLER OBJECT TYPE"
                ],
                "body": null
            }
        },
        "outputGraph": {
            "template": "outbound-calls-graph"
        },
        "children": []
    },
    {
        "name": "Message Out Descriptor Triggered by One Single Cobol Program",
        "id": 1202282,
        "category": "TASK",
        "branchType": null,
        "rationales": {
            "Agnostic": "Message Output Descriptor with Single access are less complex to review or replace \nEach Cobol program Calling the MOD must be carefully investigated.\nWill be also recommended to check if the same program is connected to a communication or data access layer.\n\n\n\n"
        },
        "remediations": {},
        "populations": {
            "Mainframe": "CAST_IMS_MessageOutputDescriptor Called by Cobol Program  And Number of CALLER COBOL Program =1"
        },
        "description": "This rule identifies IMS Message Output Descriptor in mainframe applications Called by One Single Cobol Program",
        "impacts": [
            "rehost"
        ],
        "effort": "low",
        "parents": [
            1202278
        ],
        "outputTable": {
            "template": "outbound-calls-table",
            "table": {
                "head": [
                    "IMS MOD FULLNAME",
                    "CALLER FULLNAME",
                    "CALLER OBJECT-TYPE"
                ],
                "body": null
            }
        },
        "outputGraph": {
            "template": "outbound-calls-graph"
        },
        "children": []
    },
    {
        "name": "CICS transactions triggered by Cobol Program",
        "id": 1202315,
        "category": "TASK",
        "branchType": null,
        "rationales": {
            "Agnostic": "Identifying and analyzing the existing CICS transactions allows the migration team to assess the scope and complexity of the replacement Entry points in used to start a CICS transaction and run associated programs.\n\nTransactional programs will require code changes to handle web communication\n\nNote also that, transactional programs are good candidates to be refactored and transformed into Lambda function.\n\n"
        },
        "remediations": {},
        "populations": {
            "Mainframe": "CICS_TRANSACTION Called by Caobol Program"
        },
        "description": "This rule identifies COBOL programs Callin by CICS Transactions",
        "impacts": [
            "retire",
            "review"
        ],
        "effort": "low",
        "parents": [
            1202316
        ],
        "outputTable": {
            "template": "none",
            "table": {
                "head": [
                    "CICS TRANSACTION",
                    "CALLER FULLNAME",
                    "CALLER TYPE"
                ],
                "body": null
            }
        },
        "outputGraph": {
            "template": "none"
        },
        "children": []
    }
]